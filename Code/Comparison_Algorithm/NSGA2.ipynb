{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20692c9f-8391-464e-a215-c50deb14a48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: Best Reward = 608.6084\n",
      "Generation 2: Best Reward = 646.5010\n",
      "Generation 3: Best Reward = 763.0102\n",
      "Generation 4: Best Reward = 763.0102\n",
      "Generation 5: Best Reward = 763.0102\n",
      "Generation 6: Best Reward = 784.7495\n",
      "Generation 7: Best Reward = 784.7495\n",
      "Generation 8: Best Reward = 784.7495\n",
      "Generation 9: Best Reward = 808.4484\n",
      "Generation 10: Best Reward = 809.1660\n",
      "Generation 11: Best Reward = 878.4088\n",
      "Generation 12: Best Reward = 888.1580\n",
      "Generation 13: Best Reward = 905.7545\n",
      "Generation 14: Best Reward = 905.7545\n",
      "Generation 15: Best Reward = 981.3541\n",
      "Generation 16: Best Reward = 981.3541\n",
      "Generation 17: Best Reward = 981.3541\n",
      "Generation 18: Best Reward = 990.0145\n",
      "Generation 19: Best Reward = 1004.4786\n",
      "Generation 20: Best Reward = 1047.8822\n",
      "Generation 21: Best Reward = 1063.7782\n",
      "Generation 22: Best Reward = 1063.7782\n",
      "Generation 23: Best Reward = 1063.7782\n",
      "Generation 24: Best Reward = 1063.7782\n",
      "Generation 25: Best Reward = 1073.0249\n",
      "Generation 26: Best Reward = 1073.0249\n",
      "Generation 27: Best Reward = 1073.0249\n",
      "Generation 28: Best Reward = 1073.0249\n",
      "Generation 29: Best Reward = 1073.0249\n",
      "Generation 30: Best Reward = 1105.4824\n",
      "\n",
      "Final Best Action Sequence:\n",
      "[[1.00800709e-01]\n",
      " [4.49752273e-01]\n",
      " [3.59229760e-03]\n",
      " [1.85176720e-01]\n",
      " [1.00196053e-01]\n",
      " [8.62200550e-02]\n",
      " [6.21177131e-07]\n",
      " [2.00744476e-02]\n",
      " [6.56056795e-03]\n",
      " [1.02185352e-02]\n",
      " [8.51546756e-05]\n",
      " [8.93937809e-02]\n",
      " [6.99030996e-02]\n",
      " [1.24338109e-03]\n",
      " [1.44010112e-03]\n",
      " [3.94962840e-02]\n",
      " [5.10107863e-04]\n",
      " [1.56138046e-04]\n",
      " [1.93957216e-04]\n",
      " [7.72056558e-02]\n",
      " [3.30483626e-04]\n",
      " [8.46436689e-02]\n",
      " [7.15514048e-02]\n",
      " [1.18344481e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from chargenv import Env\n",
    "\n",
    "\n",
    "def fitness_function(environment, action_sequence):\n",
    "    total_reward = 0\n",
    "    environment.reset()\n",
    "    for i in range(len(action_sequence)):\n",
    "        state, reward, done = environment.step(action_sequence[i])\n",
    "        total_reward += reward.item()  # 确保 reward 是标量\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "class Individual:\n",
    "    def __init__(self, x, fitness_func):\n",
    "        self.x = x\n",
    "        self.f = fitness_func(x)\n",
    "        self.rank = None\n",
    "        self.crowding_distance = 0\n",
    "\n",
    "def tournament_selection(pop, k=2):\n",
    "    participants = random.sample(pop, k)\n",
    "    return max(participants, key=lambda ind: ind.f)\n",
    "\n",
    "def sbx_crossover(parent1, parent2, eta=30):\n",
    "    child1 = np.copy(parent1.x)\n",
    "    child2 = np.copy(parent2.x)\n",
    "    for i in range(child1.shape[0]):\n",
    "        for j in range(child1.shape[1]):\n",
    "            if random.random() <= 0.5:\n",
    "                x1, x2 = parent1.x[i, j], parent2.x[i, j]\n",
    "                if abs(x1 - x2) > 1e-14:\n",
    "                    x_min, x_max = min(x1, x2), max(x1, x2)\n",
    "                    rand = random.random()\n",
    "                    beta = 1.0 + 2.0 * (x_min - 0.0) / (x_max - x_min)\n",
    "                    alpha = 2.0 - beta**-(eta + 1)\n",
    "                    if rand <= 1.0 / alpha:\n",
    "                        betaq = (rand * alpha)**(1.0 / (eta + 1))\n",
    "                    else:\n",
    "                        betaq = (1.0 / (2.0 - rand * alpha))**(1.0 / (eta + 1))\n",
    "                    c1 = 0.5 * ((x_min + x_max) - betaq * (x_max - x_min))\n",
    "                    c2 = 0.5 * ((x_min + x_max) + betaq * (x_max - x_min))\n",
    "                    child1[i, j] = np.clip(c1, 0.0, 0.5)\n",
    "                    child2[i, j] = np.clip(c2, 0.0, 0.5)\n",
    "    return child1, child2\n",
    "\n",
    "def polynomial_mutation(x, mutation_rate=0.1, eta=20):\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if random.random() < mutation_rate:\n",
    "                r = random.random()\n",
    "                delta = (\n",
    "                    (2 * r)**(1.0 / (eta + 1)) - 1 if r < 0.5\n",
    "                    else 1 - (2 * (1 - r))**(1.0 / (eta + 1))\n",
    "                )\n",
    "                x[i, j] = np.clip(x[i, j] + delta, 0.0, 0.5)\n",
    "    return x\n",
    "\n",
    "def run_nsga2_single_objective(fitness_function, env, action_dim, seq_len=10, pop_size=50, generations=50):\n",
    "    def create_individual():\n",
    "        x = np.random.uniform(0, 0.5, (seq_len, action_dim))\n",
    "        return Individual(x, lambda a_seq: fitness_function(env, a_seq))\n",
    "\n",
    "    population = [create_individual() for _ in range(pop_size)]\n",
    "\n",
    "    for gen in range(generations):\n",
    "        offspring = []\n",
    "        while len(offspring) < pop_size:\n",
    "            p1 = tournament_selection(population)\n",
    "            p2 = tournament_selection(population)\n",
    "            c1_x, c2_x = sbx_crossover(p1, p2)\n",
    "            c1_x = polynomial_mutation(c1_x)\n",
    "            c2_x = polynomial_mutation(c2_x)\n",
    "            offspring.append(Individual(c1_x, lambda a_seq: fitness_function(env, a_seq)))\n",
    "            offspring.append(Individual(c2_x, lambda a_seq: fitness_function(env, a_seq)))\n",
    "\n",
    "        population.extend(offspring)\n",
    "        population.sort(key=lambda ind: ind.f, reverse=True)\n",
    "        population = population[:pop_size]\n",
    "\n",
    "        best = population[0]\n",
    "        print(f\"Generation {gen+1}: Best Reward = {best.f:.4f}\")\n",
    "\n",
    "    return population[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arrival_rate = 5\n",
    "    data_path = './datasets/EVCD{}.csv'.format(arrival_rate)\n",
    "    env = Env(1, 0, 0, arrival_rate, data_path)\n",
    "    best_ind = run_nsga2_single_objective(\n",
    "        fitness_function=fitness_function,\n",
    "        env=env,\n",
    "        action_dim=env.n_cs,\n",
    "        seq_len=24,\n",
    "        pop_size=50,\n",
    "        generations=30\n",
    "    )\n",
    "\n",
    "    print(\"\\nFinal Best Action Sequence:\")\n",
    "    print(best_ind.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588056ef-dd5f-42c1-86fb-752cc8ad49f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running NSGA-II multiple times to evaluate average performance...\n",
      "\n",
      "\n",
      "Trial 1\n",
      "[7063.24s] Best Reward = 469.2646\n",
      "[7064.24s] Best Reward = 469.2646\n",
      "[7065.24s] Best Reward = 469.2646\n",
      "[7066.24s] Best Reward = 469.2646\n",
      "[7067.25s] Best Reward = 469.2646\n",
      "Generation 1: Best Reward = 751.4858\n",
      "[7068.25s] Best Reward = 469.2646\n",
      "[7069.26s] Best Reward = 469.2646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_TRIALS):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m     best_ind \u001b[38;5;241m=\u001b[39m \u001b[43mrun_nsga2_single_objective\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfitness_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfitness_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_cs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     mean_reward, std_reward \u001b[38;5;241m=\u001b[39m evaluate_action_sequence(env, best_ind\u001b[38;5;241m.\u001b[39mx, repeat\u001b[38;5;241m=\u001b[39mTEST_REPEAT)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> Trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Mean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 79\u001b[0m, in \u001b[0;36mrun_nsga2_single_objective\u001b[0;34m(fitness_function, env, action_dim, seq_len, pop_size, generations)\u001b[0m\n\u001b[1;32m     77\u001b[0m     c2_x \u001b[38;5;241m=\u001b[39m polynomial_mutation(c2_x)\n\u001b[1;32m     78\u001b[0m     offspring\u001b[38;5;241m.\u001b[39mappend(Individual(c1_x, \u001b[38;5;28;01mlambda\u001b[39;00m a_seq: fitness_function(env, a_seq)))\n\u001b[0;32m---> 79\u001b[0m     offspring\u001b[38;5;241m.\u001b[39mappend(\u001b[43mIndividual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc2_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma_seq\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     81\u001b[0m population\u001b[38;5;241m.\u001b[39mextend(offspring)\n\u001b[1;32m     82\u001b[0m population\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m ind: ind\u001b[38;5;241m.\u001b[39mf, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mIndividual.__init__\u001b[0;34m(self, x, fitness_func)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, fitness_func):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfitness_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrowding_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[8], line 79\u001b[0m, in \u001b[0;36mrun_nsga2_single_objective.<locals>.<lambda>\u001b[0;34m(a_seq)\u001b[0m\n\u001b[1;32m     77\u001b[0m     c2_x \u001b[38;5;241m=\u001b[39m polynomial_mutation(c2_x)\n\u001b[1;32m     78\u001b[0m     offspring\u001b[38;5;241m.\u001b[39mappend(Individual(c1_x, \u001b[38;5;28;01mlambda\u001b[39;00m a_seq: fitness_function(env, a_seq)))\n\u001b[0;32m---> 79\u001b[0m     offspring\u001b[38;5;241m.\u001b[39mappend(Individual(c2_x, \u001b[38;5;28;01mlambda\u001b[39;00m a_seq: \u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_seq\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     81\u001b[0m population\u001b[38;5;241m.\u001b[39mextend(offspring)\n\u001b[1;32m     82\u001b[0m population\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m ind: ind\u001b[38;5;241m.\u001b[39mf, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mfitness_function\u001b[0;34m(environment, action_sequence)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitness_function\u001b[39m(environment, action_sequence):\n\u001b[1;32m      7\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(action_sequence)):\n\u001b[1;32m     10\u001b[0m         state, reward, done \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstep(action_sequence[i])\n",
      "File \u001b[0;32m~/autodl-tmp/NEW/chargenv.py:136\u001b[0m, in \u001b[0;36mEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevs \u001b[38;5;241m=\u001b[39m \u001b[43mEV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEVS\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# self.ev = []\u001b[39;00m\n\u001b[1;32m    138\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n",
      "File \u001b[0;32m~/autodl-tmp/NEW/EV.py:109\u001b[0m, in \u001b[0;36mEVS.__init__\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path \u001b[38;5;241m=\u001b[39m data_path\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevs: Dict[\u001b[38;5;28mstr\u001b[39m, EV] \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# 存储所有车辆\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/NEW/EV.py:115\u001b[0m, in \u001b[0;36mEVS.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"加载并解析CSV数据\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path)\n\u001b[0;32m--> 115\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 将时间字符串转换为datetime对象\u001b[39;49;00m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnection_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconnectionTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdone_charging_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdoneChargingTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[1;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/construction.py:606\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    604\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 606\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    608\u001b[0m         object_index\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[1;32m    611\u001b[0m     ):\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[1;32m    613\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:1195\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m     value,\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m#  numpy would have done it for us.\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m     convert_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1194\u001b[0m     convert_non_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m-> 1195\u001b[0m     dtype_if_all_nat\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mM8[ns]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1196\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7070.26s] Best Reward = 469.2646\n",
      "[7071.26s] Best Reward = 469.2646\n",
      "[7072.26s] Best Reward = 469.2646\n",
      "[7073.26s] Best Reward = 469.2646\n",
      "[7074.26s] Best Reward = 469.2646\n",
      "[7075.26s] Best Reward = 469.2646\n",
      "[7076.27s] Best Reward = 469.2646\n",
      "[7077.27s] Best Reward = 469.2646\n",
      "[7078.27s] Best Reward = 469.2646\n",
      "[7079.27s] Best Reward = 469.2646\n",
      "[7080.27s] Best Reward = 469.2646\n",
      "[7081.27s] Best Reward = 469.2646\n",
      "[7082.27s] Best Reward = 469.2646\n",
      "[7083.27s] Best Reward = 469.2646\n",
      "[7084.27s] Best Reward = 469.2646\n",
      "[7085.27s] Best Reward = 469.2646\n",
      "[7086.27s] Best Reward = 469.2646\n",
      "[7087.27s] Best Reward = 469.2646\n",
      "[7088.27s] Best Reward = 469.2646\n",
      "[7089.27s] Best Reward = 469.2646\n",
      "[7090.28s] Best Reward = 469.2646\n",
      "[7091.28s] Best Reward = 469.2646\n",
      "[7092.28s] Best Reward = 469.2646\n",
      "[7093.28s] Best Reward = 469.2646\n",
      "[7094.28s] Best Reward = 469.2646\n",
      "[7095.28s] Best Reward = 469.2646\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from chargenv import Env\n",
    "import time\n",
    "\n",
    "def fitness_function(environment, action_sequence):\n",
    "    total_reward = 0\n",
    "    environment.reset()\n",
    "    for i in range(len(action_sequence)):\n",
    "        state, reward, done = environment.step(action_sequence[i])\n",
    "        total_reward += reward.item()  # 确保 reward 是标量\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "class Individual:\n",
    "    def __init__(self, x, fitness_func):\n",
    "        self.x = x\n",
    "        self.f = fitness_func(x)\n",
    "        self.rank = None\n",
    "        self.crowding_distance = 0\n",
    "\n",
    "def tournament_selection(pop, k=2):\n",
    "    participants = random.sample(pop, k)\n",
    "    return max(participants, key=lambda ind: ind.f)\n",
    "\n",
    "def sbx_crossover(parent1, parent2, eta=30):\n",
    "    child1 = np.copy(parent1.x)\n",
    "    child2 = np.copy(parent2.x)\n",
    "    for i in range(child1.shape[0]):\n",
    "        for j in range(child1.shape[1]):\n",
    "            if random.random() <= 0.5:\n",
    "                x1, x2 = parent1.x[i, j], parent2.x[i, j]\n",
    "                if abs(x1 - x2) > 1e-14:\n",
    "                    x_min, x_max = min(x1, x2), max(x1, x2)\n",
    "                    rand = random.random()\n",
    "                    beta = 1.0 + 2.0 * (x_min - 0.0) / (x_max - x_min)\n",
    "                    alpha = 2.0 - beta**-(eta + 1)\n",
    "                    if rand <= 1.0 / alpha:\n",
    "                        betaq = (rand * alpha)**(1.0 / (eta + 1))\n",
    "                    else:\n",
    "                        betaq = (1.0 / (2.0 - rand * alpha))**(1.0 / (eta + 1))\n",
    "                    c1 = 0.5 * ((x_min + x_max) - betaq * (x_max - x_min))\n",
    "                    c2 = 0.5 * ((x_min + x_max) + betaq * (x_max - x_min))\n",
    "                    child1[i, j] = np.clip(c1, 0.0, 0.5)\n",
    "                    child2[i, j] = np.clip(c2, 0.0, 0.5)\n",
    "    return child1, child2\n",
    "\n",
    "def polynomial_mutation(x, mutation_rate=0.1, eta=20):\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if random.random() < mutation_rate:\n",
    "                r = random.random()\n",
    "                delta = (\n",
    "                    (2 * r)**(1.0 / (eta + 1)) - 1 if r < 0.5\n",
    "                    else 1 - (2 * (1 - r))**(1.0 / (eta + 1))\n",
    "                )\n",
    "                x[i, j] = np.clip(x[i, j] + delta, 0.0, 0.5)\n",
    "    return x\n",
    "def run_nsga2_single_objective(fitness_function, env, action_dim, seq_len=10, pop_size=50, generations=50):\n",
    "    start_time = time.time()  # ← 加在函数最前面\n",
    "\n",
    "    def create_individual():\n",
    "        x = np.random.uniform(0, 0.5, (seq_len, action_dim))\n",
    "        return Individual(x, lambda a_seq: fitness_function(env, a_seq))\n",
    "\n",
    "    population = [create_individual() for _ in range(pop_size)]\n",
    "\n",
    "    for gen in range(generations):\n",
    "        offspring = []\n",
    "        while len(offspring) < pop_size:\n",
    "            p1 = tournament_selection(population)\n",
    "            p2 = tournament_selection(population)\n",
    "            c1_x, c2_x = sbx_crossover(p1, p2)\n",
    "            c1_x = polynomial_mutation(c1_x)\n",
    "            c2_x = polynomial_mutation(c2_x)\n",
    "            offspring.append(Individual(c1_x, lambda a_seq: fitness_function(env, a_seq)))\n",
    "            offspring.append(Individual(c2_x, lambda a_seq: fitness_function(env, a_seq)))\n",
    "\n",
    "        population.extend(offspring)\n",
    "        population.sort(key=lambda ind: ind.f, reverse=True)\n",
    "        population = population[:pop_size]\n",
    "\n",
    "        best = population[0]\n",
    "        print(f\"Generation {gen+1}: Best Reward = {best.f:.4f}\")\n",
    "\n",
    "    duration = time.time() - start_time  # ← 计算耗时\n",
    "    print(f\"NSGA-II run finished in {duration:.2f} seconds.\")\n",
    "\n",
    "    return population[0]\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_action_sequence(env, action_sequence, repeat=10):\n",
    "    \"\"\"重复执行某个动作序列，计算平均奖励\"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(repeat):\n",
    "        rewards.append(fitness_function(env, action_sequence))\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arrival_rate = 3\n",
    "    data_path = './datasets/EVCD{}.csv'.format(arrival_rate)\n",
    "    env = Env(1, 0, 0, arrival_rate, data_path)\n",
    "\n",
    "    NUM_TRIALS = 5\n",
    "    TEST_REPEAT = 100\n",
    "\n",
    "    all_means = []\n",
    "    all_stds = []\n",
    "\n",
    "    print(\"\\nRunning NSGA-II multiple times to evaluate average performance...\\n\")\n",
    "\n",
    "    total_start_time = time.time()  # ← 添加总开始时间\n",
    "\n",
    "    for i in range(NUM_TRIALS):\n",
    "        print(f\"\\nTrial {i+1}\")\n",
    "        best_ind = run_nsga2_single_objective(\n",
    "            fitness_function=fitness_function,\n",
    "            env=env,\n",
    "            action_dim=env.n_cs,\n",
    "            seq_len=24,\n",
    "            pop_size=30,\n",
    "            generations=100\n",
    "        )\n",
    "\n",
    "        mean_reward, std_reward = evaluate_action_sequence(env, best_ind.x, repeat=TEST_REPEAT)\n",
    "        print(f\">>> Trial {i+1} | Mean reward: {mean_reward:.2f}, Std: {std_reward:.2f}\")\n",
    "        all_means.append(mean_reward)\n",
    "        all_stds.append(std_reward)\n",
    "\n",
    "    total_duration = time.time() - total_start_time  # ← 总耗时\n",
    "\n",
    "    overall_mean = np.mean(all_means)\n",
    "    overall_std = np.std(all_means)\n",
    "\n",
    "    print(\"\\n===== NSGA-II Evaluation Summary =====\")\n",
    "    print(f\"Average of mean rewards over {NUM_TRIALS} trials: {overall_mean:.4f}\")\n",
    "    print(f\"Standard deviation across trials: {overall_std:.4f}\")\n",
    "    print(f\"Total time for all {NUM_TRIALS} trials: {total_duration:.2f} seconds\")\n",
    "    print(f\"Average time per trial: {total_duration / NUM_TRIALS:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4434e5-43a3-4580-8f6e-b5843d97c18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running NSGA-II multiple times to evaluate average performance...\n",
      "\n",
      "\n",
      "Trial 1\n",
      "[7.69s] Best Reward = 472.1435\n",
      "[8.69s] Best Reward = 472.1435\n",
      "[9.69s] Best Reward = 472.1435\n",
      "[10.70s] Best Reward = 472.1435\n",
      "[11.70s] Best Reward = 523.3223\n",
      "[12.70s] Best Reward = 523.3223\n",
      "[13.70s] Best Reward = 523.3223\n",
      "[14.70s] Best Reward = 523.3223\n",
      "[15.70s] Best Reward = 523.3223\n",
      "[16.70s] Best Reward = 523.3223\n",
      "[17.70s] Best Reward = 536.1617\n",
      "[18.70s] Best Reward = 536.1617\n",
      "[19.70s] Best Reward = 536.1617\n",
      "[20.70s] Best Reward = 536.1617\n",
      "[21.71s] Best Reward = 536.1617\n",
      "[22.76s] Best Reward = 536.1617\n",
      "[23.76s] Best Reward = 536.1617\n",
      "[24.76s] Best Reward = 536.1617\n",
      "[25.76s] Best Reward = 536.1617\n",
      "[26.76s] Best Reward = 536.1617\n",
      "[27.76s] Best Reward = 542.2917\n",
      "[28.76s] Best Reward = 542.2917\n",
      "[29.76s] Best Reward = 542.2917\n",
      "[30.77s] Best Reward = 542.2917\n",
      "[31.77s] Best Reward = 543.1985\n",
      "[32.77s] Best Reward = 543.1985\n",
      "[33.77s] Best Reward = 543.1985\n",
      "[34.78s] Best Reward = 543.1985\n",
      "[35.78s] Best Reward = 543.1985\n",
      "[36.78s] Best Reward = 544.0776\n",
      "[37.78s] Best Reward = 544.0776\n",
      "[38.78s] Best Reward = 544.0776\n",
      "[39.79s] Best Reward = 544.0776\n",
      "[40.79s] Best Reward = 544.0776\n",
      "[41.79s] Best Reward = 544.0776\n",
      "[42.79s] Best Reward = 544.0776\n",
      "[43.79s] Best Reward = 544.0776\n",
      "[44.79s] Best Reward = 544.0776\n",
      "[45.79s] Best Reward = 544.0776\n",
      "[46.80s] Best Reward = 544.0776\n",
      "[47.80s] Best Reward = 544.0776\n",
      "[48.80s] Best Reward = 544.0776\n",
      "[49.86s] Best Reward = 544.0776\n",
      "[50.86s] Best Reward = 544.6114\n",
      "[51.86s] Best Reward = 544.6114\n",
      "[52.86s] Best Reward = 544.6114\n",
      "[53.86s] Best Reward = 544.6114\n",
      "[54.86s] Best Reward = 544.6114\n",
      "[55.86s] Best Reward = 544.6114\n",
      "[56.87s] Best Reward = 544.6114\n",
      "[57.87s] Best Reward = 544.6114\n",
      "[58.88s] Best Reward = 544.6114\n",
      "[59.89s] Best Reward = 544.6114\n",
      "[60.89s] Best Reward = 544.6114\n",
      "[61.89s] Best Reward = 544.6114\n",
      "[62.90s] Best Reward = 544.6114\n",
      "[63.90s] Best Reward = 544.6114\n",
      "[64.91s] Best Reward = 544.6114\n",
      "[65.91s] Best Reward = 544.6114\n",
      "[66.91s] Best Reward = 544.6114\n",
      "[67.91s] Best Reward = 544.6114\n",
      "[68.91s] Best Reward = 544.6114\n",
      "[69.96s] Best Reward = 544.6114\n",
      "[70.96s] Best Reward = 544.6114\n",
      "[71.96s] Best Reward = 544.6114\n",
      "[72.96s] Best Reward = 544.6114\n",
      "[73.96s] Best Reward = 549.4267\n",
      "[74.96s] Best Reward = 549.4267\n",
      "[75.96s] Best Reward = 549.4267\n",
      "[76.97s] Best Reward = 549.4267\n",
      "[77.97s] Best Reward = 549.4267\n",
      "[78.98s] Best Reward = 553.5202\n",
      "[79.98s] Best Reward = 553.5202\n",
      "[80.99s] Best Reward = 553.5202\n",
      "[82.00s] Best Reward = 553.5202\n",
      "[83.00s] Best Reward = 553.5202\n",
      "[84.01s] Best Reward = 553.7091\n",
      "[85.06s] Best Reward = 553.7091\n",
      "[86.07s] Best Reward = 553.7091\n",
      "[87.08s] Best Reward = 553.7091\n",
      "[88.08s] Best Reward = 553.7091\n",
      "[89.08s] Best Reward = 553.7091\n",
      "[90.08s] Best Reward = 553.7091\n",
      "[91.08s] Best Reward = 553.7091\n",
      "[92.08s] Best Reward = 553.7091\n",
      "[93.09s] Best Reward = 554.5377\n",
      "[94.09s] Best Reward = 554.5377\n",
      "[95.09s] Best Reward = 554.5377\n",
      "[96.10s] Best Reward = 554.5377\n",
      "[97.16s] Best Reward = 554.5377\n",
      "[98.16s] Best Reward = 554.5377\n",
      "[99.16s] Best Reward = 554.5377\n",
      "[100.16s] Best Reward = 554.5377\n",
      "[101.17s] Best Reward = 554.5377\n",
      "[102.17s] Best Reward = 554.5377\n",
      "[103.17s] Best Reward = 554.5377\n",
      "[104.17s] Best Reward = 556.3081\n",
      "[105.17s] Best Reward = 556.3081\n",
      "[106.17s] Best Reward = 556.3081\n",
      "[107.17s] Best Reward = 556.3081\n",
      "[108.17s] Best Reward = 556.3081\n",
      "[109.18s] Best Reward = 556.3081\n",
      "[110.18s] Best Reward = 556.3081\n",
      "[111.18s] Best Reward = 556.3081\n",
      "[112.18s] Best Reward = 556.3081\n",
      "[113.18s] Best Reward = 556.3081\n",
      "[114.18s] Best Reward = 561.5061\n",
      "[115.19s] Best Reward = 561.5061\n",
      "[116.19s] Best Reward = 561.5061\n",
      "[117.19s] Best Reward = 561.5061\n",
      "[118.19s] Best Reward = 561.5061\n",
      "[119.19s] Best Reward = 561.5061\n",
      "[120.19s] Best Reward = 561.5061\n",
      "[121.19s] Best Reward = 561.5061\n",
      "[122.20s] Best Reward = 561.5061\n",
      "[123.20s] Best Reward = 561.5061\n",
      "[124.20s] Best Reward = 561.5061\n",
      "[125.21s] Best Reward = 561.5061\n",
      "[126.26s] Best Reward = 561.5061\n",
      "[127.26s] Best Reward = 561.5061\n",
      "[128.26s] Best Reward = 561.5061\n",
      "[129.26s] Best Reward = 561.5061\n",
      "[130.27s] Best Reward = 561.5061\n",
      "[131.28s] Best Reward = 561.5061\n",
      "[132.29s] Best Reward = 561.5061\n",
      "[133.29s] Best Reward = 561.5061\n",
      "[134.30s] Best Reward = 561.5061\n",
      "[135.36s] Best Reward = 561.5061\n",
      "[136.36s] Best Reward = 561.5061\n",
      "[137.36s] Best Reward = 561.5061\n",
      "[138.36s] Best Reward = 561.5061\n",
      "[139.36s] Best Reward = 561.5061\n",
      "[140.36s] Best Reward = 561.5061\n",
      "[141.37s] Best Reward = 561.5061\n",
      "[142.37s] Best Reward = 561.5061\n",
      "[143.37s] Best Reward = 561.5061\n",
      "[144.38s] Best Reward = 561.5061\n",
      "[145.38s] Best Reward = 561.5061\n",
      "[146.38s] Best Reward = 561.5061\n",
      "[147.38s] Best Reward = 561.5061\n",
      "[148.39s] Best Reward = 561.5061\n",
      "[149.39s] Best Reward = 561.5061\n",
      "[150.40s] Best Reward = 561.5061\n",
      "[151.40s] Best Reward = 561.5061\n",
      "[152.40s] Best Reward = 561.5061\n",
      "[153.40s] Best Reward = 561.5061\n",
      "[154.41s] Best Reward = 561.5061\n",
      "[155.46s] Best Reward = 561.5061\n",
      "[156.46s] Best Reward = 561.5061\n",
      "[157.46s] Best Reward = 561.5061\n",
      "[158.46s] Best Reward = 561.5061\n",
      "[159.46s] Best Reward = 561.5061\n",
      "[160.46s] Best Reward = 561.5061\n",
      "[161.47s] Best Reward = 561.5061\n",
      "[162.48s] Best Reward = 561.5061\n",
      "[163.48s] Best Reward = 561.5061\n",
      "[164.49s] Best Reward = 561.5061\n",
      "[165.49s] Best Reward = 561.5061\n",
      "[166.49s] Best Reward = 561.5061\n",
      "[167.49s] Best Reward = 561.5061\n",
      "[168.49s] Best Reward = 561.5061\n",
      "[169.49s] Best Reward = 561.5061\n",
      "[170.49s] Best Reward = 561.5061\n",
      "[171.49s] Best Reward = 561.5061\n",
      "[172.50s] Best Reward = 564.4488\n",
      "[173.50s] Best Reward = 564.4488\n",
      "[174.50s] Best Reward = 564.4488\n",
      "[175.51s] Best Reward = 564.4488\n",
      "[176.56s] Best Reward = 564.4488\n",
      "[177.56s] Best Reward = 564.4488\n",
      "[178.56s] Best Reward = 564.4488\n",
      "[179.56s] Best Reward = 564.4488\n",
      "[180.56s] Best Reward = 564.4488\n",
      "[181.56s] Best Reward = 564.4488\n",
      "[182.56s] Best Reward = 564.4488\n",
      "[183.56s] Best Reward = 564.4488\n",
      "[184.57s] Best Reward = 564.4488\n",
      "[185.57s] Best Reward = 564.4488\n",
      "[186.58s] Best Reward = 564.4488\n",
      "[187.58s] Best Reward = 564.4488\n",
      "[188.58s] Best Reward = 564.4488\n",
      "[189.58s] Best Reward = 564.4488\n",
      "[190.58s] Best Reward = 564.4488\n",
      "[191.58s] Best Reward = 564.4488\n",
      "[192.58s] Best Reward = 564.4488\n",
      "[193.58s] Best Reward = 564.4488\n",
      "[194.59s] Best Reward = 564.4488\n",
      "[195.59s] Best Reward = 564.4488\n",
      "[196.60s] Best Reward = 564.4488\n",
      "[197.60s] Best Reward = 564.4488\n",
      "[198.60s] Best Reward = 564.4488\n",
      "[199.60s] Best Reward = 564.4488\n",
      "[200.66s] Best Reward = 564.4488\n",
      "[201.67s] Best Reward = 564.4488\n",
      "[202.67s] Best Reward = 564.4488\n",
      "[203.67s] Best Reward = 564.4488\n",
      "[204.67s] Best Reward = 564.4488\n",
      "[205.67s] Best Reward = 564.4488\n",
      "[206.67s] Best Reward = 564.4488\n",
      "[207.67s] Best Reward = 564.4488\n",
      "[208.68s] Best Reward = 564.4488\n",
      "[209.68s] Best Reward = 564.4488\n",
      "[210.68s] Best Reward = 564.4488\n",
      "[211.68s] Best Reward = 564.4488\n",
      "[212.68s] Best Reward = 564.4488\n",
      "[213.68s] Best Reward = 564.4488\n",
      "[214.69s] Best Reward = 564.4488\n",
      "[215.69s] Best Reward = 564.4488\n",
      "[216.69s] Best Reward = 564.4488\n",
      "[217.69s] Best Reward = 564.4488\n",
      "[218.69s] Best Reward = 564.4488\n",
      "[219.69s] Best Reward = 564.4488\n",
      "[220.69s] Best Reward = 564.4488\n",
      "[221.70s] Best Reward = 564.4488\n",
      "[222.70s] Best Reward = 564.4488\n",
      "[223.70s] Best Reward = 564.4488\n",
      "[224.70s] Best Reward = 564.4488\n",
      "[225.71s] Best Reward = 564.4488\n",
      "[226.71s] Best Reward = 564.4488\n",
      "[227.76s] Best Reward = 564.4488\n",
      "[228.76s] Best Reward = 564.4488\n",
      "[229.76s] Best Reward = 564.4488\n",
      "[230.76s] Best Reward = 564.4488\n",
      "[231.77s] Best Reward = 564.4488\n",
      "[232.77s] Best Reward = 564.4488\n",
      "[233.77s] Best Reward = 564.4488\n",
      "[234.77s] Best Reward = 564.4488\n",
      "[235.77s] Best Reward = 564.4488\n",
      "[236.77s] Best Reward = 564.4488\n",
      "[237.77s] Best Reward = 564.4488\n",
      "[238.78s] Best Reward = 564.4488\n",
      "[239.78s] Best Reward = 564.4488\n",
      "[240.78s] Best Reward = 564.4488\n",
      "[241.78s] Best Reward = 564.4488\n",
      "[242.79s] Best Reward = 564.4488\n",
      "[243.79s] Best Reward = 564.4488\n",
      "[244.79s] Best Reward = 564.4488\n",
      "[245.80s] Best Reward = 564.4488\n",
      "[246.80s] Best Reward = 564.4488\n",
      "[247.80s] Best Reward = 564.4488\n",
      "[248.80s] Best Reward = 564.4488\n",
      "[249.80s] Best Reward = 564.4488\n",
      "[250.86s] Best Reward = 564.4488\n",
      "[251.86s] Best Reward = 564.4488\n",
      "[252.86s] Best Reward = 564.4488\n",
      "[253.86s] Best Reward = 564.4488\n",
      "[254.86s] Best Reward = 564.4488\n",
      "[255.86s] Best Reward = 564.4488\n",
      "[256.86s] Best Reward = 564.4488\n",
      "[257.87s] Best Reward = 564.4488\n",
      "[258.87s] Best Reward = 564.4488\n",
      "[259.87s] Best Reward = 564.4488\n",
      "[260.87s] Best Reward = 564.4488\n",
      "[261.87s] Best Reward = 564.4488\n",
      "[262.87s] Best Reward = 564.4488\n",
      "[263.87s] Best Reward = 564.4488\n",
      "[264.88s] Best Reward = 564.4488\n",
      "[265.88s] Best Reward = 564.4488\n",
      "[266.88s] Best Reward = 564.4488\n",
      "[267.88s] Best Reward = 564.4488\n",
      "[268.88s] Best Reward = 564.4488\n",
      "[269.88s] Best Reward = 564.4488\n",
      "[270.88s] Best Reward = 564.4488\n",
      "[271.88s] Best Reward = 564.4488\n",
      "[272.88s] Best Reward = 564.4488\n",
      "[273.89s] Best Reward = 564.4488\n",
      "[274.89s] Best Reward = 564.4488\n",
      "[275.89s] Best Reward = 564.4488\n",
      "[276.89s] Best Reward = 564.4488\n",
      "[277.89s] Best Reward = 564.4488\n",
      "[278.90s] Best Reward = 564.4488\n",
      "[279.90s] Best Reward = 564.4488\n",
      "[280.96s] Best Reward = 564.4488\n",
      "[281.96s] Best Reward = 564.4488\n",
      "[282.96s] Best Reward = 564.4488\n",
      "[283.96s] Best Reward = 564.4488\n",
      "[284.96s] Best Reward = 564.4488\n",
      "[285.96s] Best Reward = 564.4488\n",
      "[286.97s] Best Reward = 564.4488\n",
      "[287.97s] Best Reward = 564.4488\n",
      "[288.97s] Best Reward = 564.4488\n",
      "[289.97s] Best Reward = 564.4488\n",
      "[290.97s] Best Reward = 564.4488\n",
      "[291.98s] Best Reward = 564.4488\n",
      "[292.98s] Best Reward = 564.4488\n",
      "[293.98s] Best Reward = 564.4488\n",
      "[294.98s] Best Reward = 564.4488\n",
      "[295.98s] Best Reward = 564.4488\n",
      "[296.98s] Best Reward = 564.4488\n",
      "[297.99s] Best Reward = 564.4488\n",
      "[298.99s] Best Reward = 564.4488\n",
      "[299.99s] Best Reward = 564.4488\n",
      "[300.99s] Best Reward = 564.4488\n",
      "[301.99s] Best Reward = 564.4488\n",
      "[302.99s] Best Reward = 564.4488\n",
      "[303.99s] Best Reward = 564.4488\n",
      "[304.99s] Best Reward = 564.4488\n",
      "[305.99s] Best Reward = 564.4488\n",
      "[307.00s] Best Reward = 564.4488\n",
      "[308.00s] Best Reward = 564.4488\n",
      "[309.00s] Best Reward = 564.4488\n",
      "[310.06s] Best Reward = 564.4488\n",
      "[311.06s] Best Reward = 564.4488\n",
      "[312.06s] Best Reward = 564.4488\n",
      "[313.06s] Best Reward = 564.4488\n",
      "[314.06s] Best Reward = 564.4488\n",
      "[315.06s] Best Reward = 564.4488\n",
      "[316.06s] Best Reward = 564.4488\n",
      "[317.06s] Best Reward = 564.4488\n",
      "[318.07s] Best Reward = 564.4488\n",
      "[319.07s] Best Reward = 564.4488\n",
      "[320.07s] Best Reward = 564.4488\n",
      "[321.07s] Best Reward = 564.4488\n",
      "[322.08s] Best Reward = 564.4488\n",
      "[323.08s] Best Reward = 564.4488\n",
      "[324.08s] Best Reward = 564.4488\n",
      "[325.08s] Best Reward = 564.4488\n",
      "[326.09s] Best Reward = 564.4488\n",
      "[327.09s] Best Reward = 564.4488\n",
      "[328.09s] Best Reward = 564.4488\n",
      "[329.09s] Best Reward = 564.4488\n",
      "[330.09s] Best Reward = 564.4488\n",
      "[331.09s] Best Reward = 564.4488\n",
      "[332.09s] Best Reward = 564.4488\n",
      "[333.10s] Best Reward = 564.4488\n",
      "[334.10s] Best Reward = 564.4488\n",
      "[335.10s] Best Reward = 564.4488\n",
      "[336.10s] Best Reward = 564.4488\n",
      "[337.10s] Best Reward = 564.4488\n",
      "[338.10s] Best Reward = 564.4488\n",
      "[339.10s] Best Reward = 564.4488\n",
      "[340.11s] Best Reward = 564.4488\n",
      "[341.16s] Best Reward = 564.4488\n",
      "[342.16s] Best Reward = 564.4488\n",
      "[343.16s] Best Reward = 564.4488\n",
      "[344.16s] Best Reward = 564.4488\n",
      "[345.16s] Best Reward = 564.4488\n",
      "[346.16s] Best Reward = 564.4488\n",
      "[347.16s] Best Reward = 564.4488\n",
      "[348.16s] Best Reward = 564.4488\n",
      "[349.16s] Best Reward = 564.4488\n",
      "[350.17s] Best Reward = 564.4488\n",
      "[351.17s] Best Reward = 564.4488\n",
      "[352.17s] Best Reward = 564.4488\n",
      "[353.17s] Best Reward = 564.4488\n",
      "[354.17s] Best Reward = 564.4488\n",
      "[355.17s] Best Reward = 564.4488\n",
      "[356.17s] Best Reward = 564.4488\n",
      "[357.17s] Best Reward = 564.4488\n",
      "[358.18s] Best Reward = 564.4488\n",
      "[359.18s] Best Reward = 564.4488\n",
      "[360.18s] Best Reward = 564.4488\n",
      "[361.19s] Best Reward = 564.4488\n",
      "[362.19s] Best Reward = 564.4488\n",
      "[363.19s] Best Reward = 564.4488\n",
      "[364.19s] Best Reward = 564.4488\n",
      "[365.20s] Best Reward = 564.4488\n",
      "[366.20s] Best Reward = 564.4488\n",
      "[367.20s] Best Reward = 564.4488\n",
      "[368.26s] Best Reward = 564.4488\n",
      "[369.26s] Best Reward = 565.4234\n",
      "[370.26s] Best Reward = 565.4234\n",
      "[371.26s] Best Reward = 565.4234\n",
      "[372.26s] Best Reward = 565.4234\n",
      "[373.26s] Best Reward = 565.4234\n",
      "[374.26s] Best Reward = 565.4234\n",
      "[375.27s] Best Reward = 565.4234\n",
      "[376.27s] Best Reward = 565.4234\n",
      "[377.27s] Best Reward = 565.4234\n",
      "[378.27s] Best Reward = 565.4234\n",
      "[379.28s] Best Reward = 565.4234\n",
      "[380.28s] Best Reward = 565.4234\n",
      "[381.28s] Best Reward = 565.4234\n",
      "[382.28s] Best Reward = 565.4234\n",
      "[383.28s] Best Reward = 565.4234\n",
      "[384.28s] Best Reward = 565.4234\n",
      "[385.28s] Best Reward = 565.4234\n",
      "[386.28s] Best Reward = 565.4234\n",
      "[387.29s] Best Reward = 565.4234\n",
      "[388.29s] Best Reward = 565.4234\n",
      "[389.29s] Best Reward = 565.4234\n",
      "[390.29s] Best Reward = 565.4234\n",
      "[391.29s] Best Reward = 565.4234\n",
      "[392.29s] Best Reward = 565.4234\n",
      "[393.29s] Best Reward = 565.4234\n",
      "[394.29s] Best Reward = 565.4234\n",
      "[395.29s] Best Reward = 565.4234\n",
      "[396.30s] Best Reward = 565.4234\n",
      "[397.30s] Best Reward = 565.4234\n",
      "[398.30s] Best Reward = 565.4234\n",
      "[399.36s] Best Reward = 565.4234\n",
      "[400.36s] Best Reward = 565.4234\n",
      "[401.36s] Best Reward = 565.4234\n",
      "[402.36s] Best Reward = 565.4234\n",
      "[403.36s] Best Reward = 565.4234\n",
      "[404.36s] Best Reward = 565.4234\n",
      "[405.36s] Best Reward = 565.4234\n",
      "[406.36s] Best Reward = 565.4234\n",
      "[407.37s] Best Reward = 566.1159\n",
      "[408.37s] Best Reward = 566.1159\n",
      "[409.37s] Best Reward = 566.1159\n",
      "[410.37s] Best Reward = 566.1159\n",
      "[411.37s] Best Reward = 566.1159\n",
      "[412.38s] Best Reward = 566.1159\n",
      "[413.38s] Best Reward = 566.1159\n",
      "[414.38s] Best Reward = 566.1159\n",
      "[415.39s] Best Reward = 566.1159\n",
      "[416.46s] Best Reward = 566.1159\n",
      "[417.46s] Best Reward = 566.1159\n",
      "[418.46s] Best Reward = 566.1159\n",
      "[419.46s] Best Reward = 566.1159\n",
      "[420.46s] Best Reward = 566.1159\n",
      "[421.46s] Best Reward = 566.1159\n",
      "[422.46s] Best Reward = 566.1159\n",
      "[423.46s] Best Reward = 566.1159\n",
      "[424.46s] Best Reward = 566.1159\n",
      "[425.47s] Best Reward = 566.1159\n",
      "[426.47s] Best Reward = 566.1159\n",
      "[427.47s] Best Reward = 566.1159\n",
      "[428.47s] Best Reward = 566.1159\n",
      "[429.47s] Best Reward = 566.1159\n",
      "[430.48s] Best Reward = 566.1159\n",
      "[431.48s] Best Reward = 566.1159\n",
      "[432.48s] Best Reward = 566.1159\n",
      "[433.48s] Best Reward = 566.1159\n",
      "[434.48s] Best Reward = 566.1159\n",
      "[435.48s] Best Reward = 566.1159\n",
      "[436.48s] Best Reward = 566.1159\n",
      "[437.48s] Best Reward = 566.1159\n",
      "[438.48s] Best Reward = 566.1159\n",
      "[439.48s] Best Reward = 566.1159\n",
      "[440.48s] Best Reward = 566.1159\n",
      "[441.48s] Best Reward = 566.1159\n",
      "[442.49s] Best Reward = 566.1159\n",
      "[443.49s] Best Reward = 566.1159\n",
      "[444.49s] Best Reward = 566.1159\n",
      "[445.49s] Best Reward = 566.1159\n",
      "[446.49s] Best Reward = 566.1159\n",
      "[447.50s] Best Reward = 566.1159\n",
      "[448.56s] Best Reward = 566.1159\n",
      "[449.56s] Best Reward = 566.1159\n",
      "[450.56s] Best Reward = 566.1159\n",
      "[451.56s] Best Reward = 566.1159\n",
      "[452.56s] Best Reward = 566.1159\n",
      "[453.56s] Best Reward = 566.1159\n",
      "[454.56s] Best Reward = 566.1159\n",
      "[455.57s] Best Reward = 566.1159\n",
      "[456.57s] Best Reward = 566.1159\n",
      "[457.57s] Best Reward = 566.2077\n",
      "[458.57s] Best Reward = 566.2077\n",
      "[459.57s] Best Reward = 566.2077\n",
      "[460.57s] Best Reward = 566.2077\n",
      "[461.57s] Best Reward = 566.2077\n",
      "[462.57s] Best Reward = 566.2077\n",
      "[463.58s] Best Reward = 566.2077\n",
      "[464.58s] Best Reward = 566.2077\n",
      "[465.58s] Best Reward = 566.2077\n",
      "[466.58s] Best Reward = 566.2077\n",
      "[467.58s] Best Reward = 566.2077\n",
      "[468.59s] Best Reward = 566.2077\n",
      "[469.59s] Best Reward = 566.2077\n",
      "[470.59s] Best Reward = 566.2077\n",
      "[471.59s] Best Reward = 566.2077\n",
      "[472.59s] Best Reward = 566.2077\n",
      "[473.59s] Best Reward = 566.2077\n",
      "[474.60s] Best Reward = 566.2077\n",
      "[475.60s] Best Reward = 566.2077\n",
      "[476.60s] Best Reward = 566.2077\n",
      "[477.60s] Best Reward = 566.2077\n",
      "[478.60s] Best Reward = 566.2077\n",
      "[479.60s] Best Reward = 566.2077\n",
      "[480.60s] Best Reward = 566.2077\n",
      "[481.66s] Best Reward = 566.2077\n",
      "[482.66s] Best Reward = 566.2077\n",
      "[483.66s] Best Reward = 566.2077\n",
      "[484.66s] Best Reward = 566.2077\n",
      "[485.66s] Best Reward = 566.2077\n",
      "NSGA-II run finished in 486.06 seconds.\n",
      ">>> Trial 1 | Mean reward: 558.84, Std: 6.67\n",
      "\n",
      "Trial 2\n",
      "[7.60s] Best Reward = 326.3629\n",
      "[8.60s] Best Reward = 326.3629\n",
      "[9.60s] Best Reward = 326.3629\n",
      "[10.61s] Best Reward = 326.3629\n",
      "[11.61s] Best Reward = 332.4000\n",
      "[12.61s] Best Reward = 332.4000\n",
      "[13.61s] Best Reward = 332.4000\n",
      "[14.62s] Best Reward = 409.8861\n",
      "[15.62s] Best Reward = 409.8861\n",
      "[16.63s] Best Reward = 409.8861\n",
      "[17.63s] Best Reward = 409.8861\n",
      "[18.64s] Best Reward = 434.7627\n",
      "[19.64s] Best Reward = 434.7627\n",
      "[20.64s] Best Reward = 434.7627\n",
      "[21.69s] Best Reward = 434.7627\n",
      "[22.70s] Best Reward = 436.4909\n",
      "[23.70s] Best Reward = 436.4909\n",
      "[24.71s] Best Reward = 436.4909\n",
      "[25.71s] Best Reward = 436.4909\n",
      "[26.72s] Best Reward = 448.1219\n",
      "[27.72s] Best Reward = 448.1219\n",
      "[28.72s] Best Reward = 448.1219\n",
      "[29.72s] Best Reward = 448.1219\n",
      "[30.73s] Best Reward = 461.2770\n",
      "[31.73s] Best Reward = 461.2770\n",
      "[32.74s] Best Reward = 461.2770\n",
      "[33.79s] Best Reward = 461.2770\n",
      "[34.80s] Best Reward = 471.2716\n",
      "[35.80s] Best Reward = 471.2716\n",
      "[36.80s] Best Reward = 471.2716\n",
      "[37.80s] Best Reward = 471.2716\n",
      "[38.81s] Best Reward = 474.5602\n",
      "[39.82s] Best Reward = 474.5602\n",
      "[40.82s] Best Reward = 474.5602\n",
      "[41.82s] Best Reward = 474.5602\n",
      "[42.82s] Best Reward = 474.5602\n",
      "[43.82s] Best Reward = 486.6821\n",
      "[44.82s] Best Reward = 486.6821\n",
      "[45.82s] Best Reward = 486.6821\n",
      "[46.82s] Best Reward = 486.6821\n",
      "[47.83s] Best Reward = 486.6821\n",
      "[48.83s] Best Reward = 486.6821\n",
      "[49.83s] Best Reward = 486.6821\n",
      "[50.90s] Best Reward = 486.6821\n",
      "[51.90s] Best Reward = 486.6821\n",
      "[52.90s] Best Reward = 486.6821\n",
      "[53.90s] Best Reward = 486.6821\n",
      "[54.90s] Best Reward = 486.6821\n",
      "[55.91s] Best Reward = 486.6821\n",
      "[56.92s] Best Reward = 497.0234\n",
      "[57.92s] Best Reward = 497.0234\n",
      "[58.92s] Best Reward = 497.0234\n",
      "[59.92s] Best Reward = 497.0234\n",
      "[60.93s] Best Reward = 497.0234\n",
      "[61.93s] Best Reward = 497.0234\n",
      "[62.94s] Best Reward = 497.0234\n",
      "[63.99s] Best Reward = 497.0234\n",
      "[64.99s] Best Reward = 497.0234\n",
      "[65.99s] Best Reward = 497.0234\n",
      "[67.00s] Best Reward = 497.0234\n",
      "[68.00s] Best Reward = 497.0234\n",
      "[69.00s] Best Reward = 497.0234\n",
      "[70.00s] Best Reward = 497.0234\n",
      "[71.00s] Best Reward = 497.0234\n",
      "[72.01s] Best Reward = 497.0234\n",
      "[73.01s] Best Reward = 497.0234\n",
      "[74.02s] Best Reward = 497.0234\n",
      "[75.02s] Best Reward = 497.1790\n",
      "[76.03s] Best Reward = 497.1790\n",
      "[77.03s] Best Reward = 497.1790\n",
      "[78.03s] Best Reward = 497.1790\n",
      "[79.03s] Best Reward = 497.1790\n",
      "[80.04s] Best Reward = 497.1790\n",
      "[81.09s] Best Reward = 497.1790\n",
      "[82.10s] Best Reward = 497.1790\n",
      "[83.10s] Best Reward = 497.1790\n",
      "[84.10s] Best Reward = 497.1790\n",
      "[85.11s] Best Reward = 497.1790\n",
      "[86.11s] Best Reward = 497.1790\n",
      "[87.11s] Best Reward = 497.1790\n",
      "[88.11s] Best Reward = 497.1790\n",
      "[89.12s] Best Reward = 497.1790\n",
      "[90.12s] Best Reward = 497.1790\n",
      "[91.12s] Best Reward = 497.1790\n",
      "[92.13s] Best Reward = 497.1790\n",
      "[93.13s] Best Reward = 497.1790\n",
      "[94.13s] Best Reward = 497.8047\n",
      "[95.14s] Best Reward = 497.8047\n",
      "[96.14s] Best Reward = 497.8047\n",
      "[97.14s] Best Reward = 497.8047\n",
      "[98.14s] Best Reward = 497.8047\n",
      "[99.14s] Best Reward = 497.8047\n",
      "[100.19s] Best Reward = 497.8047\n",
      "[101.19s] Best Reward = 497.8047\n",
      "[102.19s] Best Reward = 497.8047\n",
      "[103.20s] Best Reward = 519.8519\n",
      "[104.20s] Best Reward = 519.8519\n",
      "[105.21s] Best Reward = 519.8519\n",
      "[106.21s] Best Reward = 519.8519\n",
      "[107.21s] Best Reward = 519.8519\n",
      "[108.21s] Best Reward = 519.8519\n",
      "[109.21s] Best Reward = 519.8519\n",
      "[110.22s] Best Reward = 519.8519\n",
      "[111.23s] Best Reward = 519.8519\n",
      "[112.23s] Best Reward = 519.8519\n",
      "[113.23s] Best Reward = 519.8519\n",
      "[114.23s] Best Reward = 519.8519\n",
      "[115.23s] Best Reward = 519.8519\n",
      "[116.23s] Best Reward = 519.8519\n",
      "[117.23s] Best Reward = 519.8519\n",
      "[118.24s] Best Reward = 519.8519\n",
      "[119.24s] Best Reward = 519.8519\n",
      "[120.29s] Best Reward = 519.8519\n",
      "[121.30s] Best Reward = 519.8519\n",
      "[122.30s] Best Reward = 519.8519\n",
      "[123.30s] Best Reward = 519.8519\n",
      "[124.30s] Best Reward = 519.8519\n",
      "[125.31s] Best Reward = 519.8519\n",
      "[126.31s] Best Reward = 519.8519\n",
      "[127.31s] Best Reward = 519.8519\n",
      "[128.31s] Best Reward = 519.8519\n",
      "[129.32s] Best Reward = 519.8519\n",
      "[130.32s] Best Reward = 519.8519\n",
      "[131.32s] Best Reward = 519.8519\n",
      "[132.32s] Best Reward = 519.8519\n",
      "[133.33s] Best Reward = 519.8519\n",
      "[134.33s] Best Reward = 519.8519\n",
      "[135.34s] Best Reward = 519.8519\n",
      "[136.34s] Best Reward = 519.8519\n",
      "[137.34s] Best Reward = 519.8519\n",
      "[138.39s] Best Reward = 519.8519\n",
      "[139.39s] Best Reward = 519.8519\n",
      "[140.39s] Best Reward = 519.8519\n",
      "[141.40s] Best Reward = 519.8519\n",
      "[142.41s] Best Reward = 519.8519\n",
      "[143.41s] Best Reward = 519.8519\n",
      "[144.41s] Best Reward = 519.8519\n",
      "[145.41s] Best Reward = 519.8519\n",
      "[146.41s] Best Reward = 519.8519\n",
      "[147.41s] Best Reward = 519.8519\n",
      "[148.41s] Best Reward = 519.8519\n",
      "[149.41s] Best Reward = 519.8519\n",
      "[150.41s] Best Reward = 519.8519\n",
      "[151.42s] Best Reward = 519.8519\n",
      "[152.42s] Best Reward = 519.8519\n",
      "[153.42s] Best Reward = 519.8519\n",
      "[154.42s] Best Reward = 519.8519\n",
      "[155.42s] Best Reward = 519.8519\n",
      "[156.42s] Best Reward = 519.8519\n",
      "[157.42s] Best Reward = 519.8519\n",
      "[158.43s] Best Reward = 519.8519\n",
      "[159.49s] Best Reward = 519.8519\n",
      "[160.49s] Best Reward = 519.8519\n",
      "[161.49s] Best Reward = 519.8519\n",
      "[162.50s] Best Reward = 521.6742\n",
      "[163.50s] Best Reward = 521.6742\n",
      "[164.50s] Best Reward = 521.6742\n",
      "[165.50s] Best Reward = 521.6742\n",
      "[166.50s] Best Reward = 521.6742\n",
      "[167.50s] Best Reward = 521.7168\n",
      "[168.51s] Best Reward = 521.7168\n",
      "[169.51s] Best Reward = 521.7168\n",
      "[170.51s] Best Reward = 521.7168\n",
      "[171.51s] Best Reward = 521.7168\n",
      "[172.51s] Best Reward = 522.5909\n",
      "[173.51s] Best Reward = 522.5909\n",
      "[174.51s] Best Reward = 522.5909\n",
      "[175.51s] Best Reward = 522.5909\n",
      "[176.52s] Best Reward = 522.5909\n",
      "[177.52s] Best Reward = 522.5909\n",
      "[178.52s] Best Reward = 522.5909\n",
      "[179.52s] Best Reward = 522.5909\n",
      "[180.52s] Best Reward = 522.5909\n",
      "[181.52s] Best Reward = 522.5909\n",
      "[182.53s] Best Reward = 522.5909\n",
      "[183.53s] Best Reward = 522.5909\n",
      "[184.53s] Best Reward = 522.5909\n",
      "[185.53s] Best Reward = 522.5909\n",
      "[186.53s] Best Reward = 522.5909\n",
      "[187.54s] Best Reward = 522.5909\n",
      "[188.54s] Best Reward = 522.5909\n",
      "[189.54s] Best Reward = 522.5909\n",
      "[190.54s] Best Reward = 522.5909\n",
      "[191.54s] Best Reward = 522.5909\n",
      "[192.59s] Best Reward = 522.5909\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_TRIALS):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m     best_ind \u001b[38;5;241m=\u001b[39m \u001b[43mrun_nsga2_single_objective\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfitness_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfitness_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_cs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     mean_reward, std_reward \u001b[38;5;241m=\u001b[39m evaluate_action_sequence(env, best_ind\u001b[38;5;241m.\u001b[39mx, repeat\u001b[38;5;241m=\u001b[39mTEST_REPEAT)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> Trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Mean reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m, in \u001b[0;36mrun_nsga2_single_objective\u001b[0;34m(fitness_function, env, action_dim, seq_len, pop_size, generations)\u001b[0m\n\u001b[1;32m     88\u001b[0m     c1_x \u001b[38;5;241m=\u001b[39m polynomial_mutation(c1_x)\n\u001b[1;32m     89\u001b[0m     c2_x \u001b[38;5;241m=\u001b[39m polynomial_mutation(c2_x)\n\u001b[0;32m---> 90\u001b[0m     offspring\u001b[38;5;241m.\u001b[39mappend(\u001b[43mIndividual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc1_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma_seq\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     91\u001b[0m     offspring\u001b[38;5;241m.\u001b[39mappend(Individual(c2_x, \u001b[38;5;28;01mlambda\u001b[39;00m a_seq: fitness_function(env, a_seq)))\n\u001b[1;32m     93\u001b[0m population\u001b[38;5;241m.\u001b[39mextend(offspring)\n",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m, in \u001b[0;36mIndividual.__init__\u001b[0;34m(self, x, fitness_func)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, fitness_func):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfitness_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrowding_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m, in \u001b[0;36mrun_nsga2_single_objective.<locals>.<lambda>\u001b[0;34m(a_seq)\u001b[0m\n\u001b[1;32m     88\u001b[0m     c1_x \u001b[38;5;241m=\u001b[39m polynomial_mutation(c1_x)\n\u001b[1;32m     89\u001b[0m     c2_x \u001b[38;5;241m=\u001b[39m polynomial_mutation(c2_x)\n\u001b[0;32m---> 90\u001b[0m     offspring\u001b[38;5;241m.\u001b[39mappend(Individual(c1_x, \u001b[38;5;28;01mlambda\u001b[39;00m a_seq: \u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_seq\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     91\u001b[0m     offspring\u001b[38;5;241m.\u001b[39mappend(Individual(c2_x, \u001b[38;5;28;01mlambda\u001b[39;00m a_seq: fitness_function(env, a_seq)))\n\u001b[1;32m     93\u001b[0m population\u001b[38;5;241m.\u001b[39mextend(offspring)\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mfitness_function\u001b[0;34m(environment, action_sequence)\u001b[0m\n\u001b[1;32m      8\u001b[0m environment\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(action_sequence)):\n\u001b[0;32m---> 10\u001b[0m     state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# 确保 reward 是标量\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/autodl-tmp/NEW/chargenv.py:159\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharging_vehicles[j]\u001b[38;5;241m.\u001b[39mremove(charging_vehicle)\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step_revenue \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.25\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprice, torch\u001b[38;5;241m.\u001b[39mtensor(charging_vehicle\u001b[38;5;241m.\u001b[39mmax_price, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;241m*\u001b[39m charging_vehicle\u001b[38;5;241m.\u001b[39mcharging_power\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharging_vehicles)):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moccupied_piles[k]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_piles[k] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaitting_queue[k]\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193.60s] Best Reward = 522.5909\n",
      "[194.60s] Best Reward = 522.5909\n",
      "[195.60s] Best Reward = 522.5909\n",
      "[196.60s] Best Reward = 522.5909\n",
      "[197.60s] Best Reward = 522.5909\n",
      "[198.60s] Best Reward = 522.5909\n",
      "[199.60s] Best Reward = 522.5909\n",
      "[200.60s] Best Reward = 522.5909\n",
      "[201.60s] Best Reward = 522.5909\n",
      "[202.61s] Best Reward = 522.5909\n",
      "[203.61s] Best Reward = 522.5909\n",
      "[204.61s] Best Reward = 522.5909\n",
      "[205.61s] Best Reward = 522.5909\n",
      "[206.61s] Best Reward = 522.5909\n",
      "[207.61s] Best Reward = 522.5909\n",
      "[208.61s] Best Reward = 522.5909\n",
      "[209.61s] Best Reward = 522.5909\n",
      "[210.61s] Best Reward = 522.5909\n",
      "[211.61s] Best Reward = 522.5909\n",
      "[212.61s] Best Reward = 522.5909\n",
      "[213.61s] Best Reward = 522.5909\n",
      "[214.61s] Best Reward = 522.5909\n",
      "[215.61s] Best Reward = 522.5909\n",
      "[216.61s] Best Reward = 522.5909\n",
      "[217.62s] Best Reward = 522.5909\n",
      "[218.62s] Best Reward = 522.5909\n",
      "[219.62s] Best Reward = 522.5909\n",
      "[220.62s] Best Reward = 522.5909\n",
      "[221.62s] Best Reward = 522.5909\n",
      "[222.62s] Best Reward = 522.5909\n",
      "[223.62s] Best Reward = 522.5909\n",
      "[224.62s] Best Reward = 522.5909\n",
      "[225.62s] Best Reward = 522.5909\n",
      "[226.62s] Best Reward = 522.5909\n",
      "[227.62s] Best Reward = 522.5909\n",
      "[228.62s] Best Reward = 522.5909\n",
      "[229.62s] Best Reward = 522.5909\n",
      "[230.62s] Best Reward = 522.5909\n",
      "[231.62s] Best Reward = 522.5909\n",
      "[232.63s] Best Reward = 522.5909\n",
      "[233.63s] Best Reward = 522.5909\n",
      "[234.63s] Best Reward = 522.5909\n",
      "[235.63s] Best Reward = 522.5909\n",
      "[236.63s] Best Reward = 522.5909\n",
      "[237.63s] Best Reward = 522.5909\n",
      "[238.63s] Best Reward = 522.5909\n",
      "[239.63s] Best Reward = 522.5909\n",
      "[240.63s] Best Reward = 522.5909\n",
      "[241.63s] Best Reward = 522.5909\n",
      "[242.64s] Best Reward = 522.5909\n",
      "[243.64s] Best Reward = 522.5909\n",
      "[244.64s] Best Reward = 522.5909\n",
      "[245.64s] Best Reward = 522.5909\n",
      "[246.64s] Best Reward = 522.5909\n",
      "[247.64s] Best Reward = 522.5909\n",
      "[248.64s] Best Reward = 522.5909\n",
      "[249.64s] Best Reward = 522.5909\n",
      "[250.64s] Best Reward = 522.5909\n",
      "[251.64s] Best Reward = 522.5909\n",
      "[252.64s] Best Reward = 522.5909\n",
      "[253.64s] Best Reward = 522.5909\n",
      "[254.64s] Best Reward = 522.5909\n",
      "[255.64s] Best Reward = 522.5909\n",
      "[256.64s] Best Reward = 522.5909\n",
      "[257.65s] Best Reward = 522.5909\n",
      "[258.65s] Best Reward = 522.5909\n",
      "[259.65s] Best Reward = 522.5909\n",
      "[260.65s] Best Reward = 522.5909\n",
      "[261.65s] Best Reward = 522.5909\n",
      "[262.65s] Best Reward = 522.5909\n",
      "[263.65s] Best Reward = 522.5909\n",
      "[264.65s] Best Reward = 522.5909\n",
      "[265.65s] Best Reward = 522.5909\n",
      "[266.65s] Best Reward = 522.5909\n",
      "[267.65s] Best Reward = 522.5909\n",
      "[268.65s] Best Reward = 522.5909\n",
      "[269.65s] Best Reward = 522.5909\n",
      "[270.65s] Best Reward = 522.5909\n",
      "[271.65s] Best Reward = 522.5909\n",
      "[272.66s] Best Reward = 522.5909\n",
      "[273.66s] Best Reward = 522.5909\n",
      "[274.66s] Best Reward = 522.5909\n",
      "[275.66s] Best Reward = 522.5909\n",
      "[276.66s] Best Reward = 522.5909\n",
      "[277.66s] Best Reward = 522.5909\n",
      "[278.66s] Best Reward = 522.5909\n",
      "[279.66s] Best Reward = 522.5909\n",
      "[280.66s] Best Reward = 522.5909\n",
      "[281.66s] Best Reward = 522.5909\n",
      "[282.66s] Best Reward = 522.5909\n",
      "[283.66s] Best Reward = 522.5909\n",
      "[284.66s] Best Reward = 522.5909\n",
      "[285.66s] Best Reward = 522.5909\n",
      "[286.66s] Best Reward = 522.5909\n",
      "[287.67s] Best Reward = 522.5909\n",
      "[288.67s] Best Reward = 522.5909\n",
      "[289.67s] Best Reward = 522.5909\n",
      "[290.67s] Best Reward = 522.5909\n",
      "[291.67s] Best Reward = 522.5909\n",
      "[292.67s] Best Reward = 522.5909\n",
      "[293.67s] Best Reward = 522.5909\n",
      "[294.67s] Best Reward = 522.5909\n",
      "[295.67s] Best Reward = 522.5909\n",
      "[296.67s] Best Reward = 522.5909\n",
      "[297.67s] Best Reward = 522.5909\n",
      "[298.67s] Best Reward = 522.5909\n",
      "[299.67s] Best Reward = 522.5909\n",
      "[300.68s] Best Reward = 522.5909\n",
      "[301.68s] Best Reward = 522.5909\n",
      "[302.68s] Best Reward = 522.5909\n",
      "[303.68s] Best Reward = 522.5909\n",
      "[304.68s] Best Reward = 522.5909\n",
      "[305.68s] Best Reward = 522.5909\n",
      "[306.68s] Best Reward = 522.5909\n",
      "[307.68s] Best Reward = 522.5909\n",
      "[308.68s] Best Reward = 522.5909\n",
      "[309.68s] Best Reward = 522.5909\n",
      "[310.68s] Best Reward = 522.5909\n",
      "[311.68s] Best Reward = 522.5909\n",
      "[312.68s] Best Reward = 522.5909\n",
      "[313.68s] Best Reward = 522.5909\n",
      "[314.68s] Best Reward = 522.5909\n",
      "[315.69s] Best Reward = 522.5909\n",
      "[316.69s] Best Reward = 522.5909\n",
      "[317.69s] Best Reward = 522.5909\n",
      "[318.69s] Best Reward = 522.5909\n",
      "[319.69s] Best Reward = 522.5909\n",
      "[320.69s] Best Reward = 522.5909\n",
      "[321.69s] Best Reward = 522.5909\n",
      "[322.69s] Best Reward = 522.5909\n",
      "[323.69s] Best Reward = 522.5909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from chargenv import Env\n",
    "import time\n",
    "import threading\n",
    "def fitness_function(environment, action_sequence):\n",
    "    total_reward = 0\n",
    "    environment.reset()\n",
    "    for i in range(len(action_sequence)):\n",
    "        state, reward, done = environment.step(action_sequence[i])\n",
    "        total_reward += reward.item()  # 确保 reward 是标量\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "class Individual:\n",
    "    def __init__(self, x, fitness_func):\n",
    "        self.x = x\n",
    "        self.f = fitness_func(x)\n",
    "        self.rank = None\n",
    "        self.crowding_distance = 0\n",
    "\n",
    "def tournament_selection(pop, k=2):\n",
    "    participants = random.sample(pop, k)\n",
    "    return max(participants, key=lambda ind: ind.f)\n",
    "\n",
    "def sbx_crossover(parent1, parent2, eta=30):\n",
    "    child1 = np.copy(parent1.x)\n",
    "    child2 = np.copy(parent2.x)\n",
    "    for i in range(child1.shape[0]):\n",
    "        for j in range(child1.shape[1]):\n",
    "            if random.random() <= 0.5:\n",
    "                x1, x2 = parent1.x[i, j], parent2.x[i, j]\n",
    "                if abs(x1 - x2) > 1e-14:\n",
    "                    x_min, x_max = min(x1, x2), max(x1, x2)\n",
    "                    rand = random.random()\n",
    "                    beta = 1.0 + 2.0 * (x_min - 0.0) / (x_max - x_min)\n",
    "                    alpha = 2.0 - beta**-(eta + 1)\n",
    "                    if rand <= 1.0 / alpha:\n",
    "                        betaq = (rand * alpha)**(1.0 / (eta + 1))\n",
    "                    else:\n",
    "                        betaq = (1.0 / (2.0 - rand * alpha))**(1.0 / (eta + 1))\n",
    "                    c1 = 0.5 * ((x_min + x_max) - betaq * (x_max - x_min))\n",
    "                    c2 = 0.5 * ((x_min + x_max) + betaq * (x_max - x_min))\n",
    "                    child1[i, j] = np.clip(c1, 0.0, 0.5)\n",
    "                    child2[i, j] = np.clip(c2, 0.0, 0.5)\n",
    "    return child1, child2\n",
    "\n",
    "def polynomial_mutation(x, mutation_rate=0.1, eta=20):\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if random.random() < mutation_rate:\n",
    "                r = random.random()\n",
    "                delta = (\n",
    "                    (2 * r)**(1.0 / (eta + 1)) - 1 if r < 0.5\n",
    "                    else 1 - (2 * (1 - r))**(1.0 / (eta + 1))\n",
    "                )\n",
    "                x[i, j] = np.clip(x[i, j] + delta, 0.0, 0.5)\n",
    "    return x\n",
    "def run_nsga2_single_objective(fitness_function, env, action_dim, seq_len=10, pop_size=50, generations=50):\n",
    "    start_time = time.time()\n",
    "    best_holder = {\"best\": None, \"stop\": False}\n",
    "\n",
    "    def create_individual():\n",
    "        x = np.random.uniform(0, 0.5, (seq_len, action_dim))\n",
    "        return Individual(x, lambda a_seq: fitness_function(env, a_seq))\n",
    "\n",
    "    population = [create_individual() for _ in range(pop_size)]\n",
    "\n",
    "    # 定义每秒输出函数\n",
    "    def print_best_reward():\n",
    "        if best_holder[\"stop\"]:\n",
    "            return\n",
    "        if best_holder[\"best\"] is not None:\n",
    "            now = time.time() - start_time\n",
    "            print(f\"[{now:.2f}s] Best Reward = {best_holder['best'].f:.4f}\")\n",
    "        threading.Timer(1.0, print_best_reward).start()\n",
    "\n",
    "    print_best_reward()  # 启动定时器\n",
    "\n",
    "    for gen in range(generations):\n",
    "        offspring = []\n",
    "        while len(offspring) < pop_size:\n",
    "            p1 = tournament_selection(population)\n",
    "            p2 = tournament_selection(population)\n",
    "            c1_x, c2_x = sbx_crossover(p1, p2)\n",
    "            c1_x = polynomial_mutation(c1_x)\n",
    "            c2_x = polynomial_mutation(c2_x)\n",
    "            offspring.append(Individual(c1_x, lambda a_seq: fitness_function(env, a_seq)))\n",
    "            offspring.append(Individual(c2_x, lambda a_seq: fitness_function(env, a_seq)))\n",
    "\n",
    "        population.extend(offspring)\n",
    "        population.sort(key=lambda ind: ind.f, reverse=True)\n",
    "        population = population[:pop_size]\n",
    "        best_holder[\"best\"] = population[0]  # 实时更新当前最优\n",
    "\n",
    "    best_holder[\"stop\"] = True  # 通知停止输出\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"NSGA-II run finished in {duration:.2f} seconds.\")\n",
    "    return population[0]\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_action_sequence(env, action_sequence, repeat=10):\n",
    "    \"\"\"重复执行某个动作序列，计算平均奖励\"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(repeat):\n",
    "        rewards.append(fitness_function(env, action_sequence))\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arrival_rate = 6\n",
    "    data_path = './datasets/EVCD{}.csv'.format(arrival_rate)\n",
    "    env = Env(1, 0, 0, arrival_rate, data_path)\n",
    "\n",
    "    NUM_TRIALS = 5\n",
    "    TEST_REPEAT = 100\n",
    "\n",
    "    all_means = []\n",
    "    all_stds = []\n",
    "\n",
    "    print(\"\\nRunning NSGA-II multiple times to evaluate average performance...\\n\")\n",
    "\n",
    "    total_start_time = time.time()  # ← 添加总开始时间\n",
    "\n",
    "    for i in range(NUM_TRIALS):\n",
    "        print(f\"\\nTrial {i+1}\")\n",
    "        best_ind = run_nsga2_single_objective(\n",
    "            fitness_function=fitness_function,\n",
    "            env=env,\n",
    "            action_dim=env.n_cs,\n",
    "            seq_len=24,\n",
    "            pop_size=30,\n",
    "            generations=100\n",
    "        )\n",
    "\n",
    "        mean_reward, std_reward = evaluate_action_sequence(env, best_ind.x, repeat=TEST_REPEAT)\n",
    "        print(f\">>> Trial {i+1} | Mean reward: {mean_reward:.2f}, Std: {std_reward:.2f}\")\n",
    "        all_means.append(mean_reward)\n",
    "        all_stds.append(std_reward)\n",
    "\n",
    "    total_duration = time.time() - total_start_time  # ← 总耗时\n",
    "\n",
    "    overall_mean = np.mean(all_means)\n",
    "    overall_std = np.std(all_means)\n",
    "\n",
    "    print(\"\\n===== NSGA-II Evaluation Summary =====\")\n",
    "    print(f\"Average of mean rewards over {NUM_TRIALS} trials: {overall_mean:.4f}\")\n",
    "    print(f\"Standard deviation across trials: {overall_std:.4f}\")\n",
    "    print(f\"Total time for all {NUM_TRIALS} trials: {total_duration:.2f} seconds\")\n",
    "    print(f\"Average time per trial: {total_duration / NUM_TRIALS:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8df09-26b9-43d7-98f3-36189731ceae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca9a31-15af-494c-a560-1c53c33ddaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
