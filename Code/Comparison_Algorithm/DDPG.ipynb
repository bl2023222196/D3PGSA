{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835a6d49-fbcb-4d85-94d0-588a38133b86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodes: 10, reward: 177.081497\n",
      "episodes: 20, reward: 109.968953\n",
      "episodes: 30, reward: 208.811878\n",
      "episodes: 40, reward: 188.276413\n",
      "episodes: 50, reward: 152.703500\n",
      "episodes: 60, reward: 135.768466\n",
      "episodes: 70, reward: 148.563847\n",
      "episodes: 80, reward: 147.357442\n",
      "episodes: 90, reward: 122.924521\n",
      "episodes: 100, reward: 123.486603\n",
      "episodes: 110, reward: 139.247587\n",
      "episodes: 120, reward: 137.008859\n",
      "episodes: 130, reward: 140.488387\n",
      "episodes: 140, reward: 132.708817\n",
      "episodes: 150, reward: 131.878316\n",
      "episodes: 160, reward: 126.551942\n",
      "episodes: 170, reward: 131.396768\n",
      "episodes: 180, reward: 130.478831\n",
      "episodes: 190, reward: 135.964710\n",
      "episodes: 200, reward: 138.747390\n",
      "episodes: 210, reward: 128.290901\n",
      "episodes: 220, reward: 599.137426\n",
      "episodes: 230, reward: 956.639980\n",
      "episodes: 240, reward: 979.443796\n",
      "episodes: 250, reward: 1007.066318\n",
      "episodes: 260, reward: 971.005488\n",
      "episodes: 270, reward: 1017.417157\n",
      "episodes: 280, reward: 1048.362604\n",
      "episodes: 290, reward: 1052.698631\n",
      "episodes: 300, reward: 1036.666136\n",
      "episodes: 310, reward: 1051.115373\n",
      "episodes: 320, reward: 1069.937856\n",
      "episodes: 330, reward: 1066.334169\n",
      "episodes: 340, reward: 1081.806157\n",
      "episodes: 350, reward: 1077.155295\n",
      "episodes: 360, reward: 1070.259550\n",
      "episodes: 370, reward: 1072.570827\n",
      "episodes: 380, reward: 1096.442942\n",
      "episodes: 390, reward: 1084.419248\n",
      "episodes: 400, reward: 1088.518682\n",
      "episodes: 410, reward: 1096.813809\n",
      "episodes: 420, reward: 1084.599227\n",
      "episodes: 430, reward: 1085.303167\n",
      "episodes: 440, reward: 1091.162066\n",
      "episodes: 450, reward: 1083.505351\n",
      "episodes: 460, reward: 1084.499799\n",
      "episodes: 470, reward: 1108.434292\n",
      "episodes: 480, reward: 1090.375162\n",
      "episodes: 490, reward: 1086.697482\n",
      "episodes: 500, reward: 1098.388847\n",
      "episodes: 510, reward: 1100.176253\n",
      "episodes: 520, reward: 1101.666710\n",
      "episodes: 530, reward: 1105.422041\n",
      "episodes: 540, reward: 1101.654304\n",
      "episodes: 550, reward: 1086.309891\n",
      "episodes: 560, reward: 1104.341243\n",
      "episodes: 570, reward: 1102.632844\n",
      "episodes: 580, reward: 1115.373827\n",
      "episodes: 590, reward: 1099.716673\n",
      "episodes: 600, reward: 1107.221062\n",
      "episodes: 610, reward: 1110.868511\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 218\u001b[0m\n\u001b[1;32m    215\u001b[0m action_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# 动作最大值\u001b[39;00m\n\u001b[1;32m    216\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPG(state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device)\n\u001b[0;32m--> 218\u001b[0m return_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_off_policy_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminimal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# episodes_list = list(range(len(return_list)))\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# plt.plot(episodes_list, return_list)\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# plt.xlabel('Episodes')\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# plt.ylabel('Returns')\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# plt.title('DDPG on DY')\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result/ddpg\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(arrival_rate), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn[7], line 152\u001b[0m, in \u001b[0;36mtrain_off_policy_agent\u001b[0;34m(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m    151\u001b[0m     episode_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 152\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     agent\u001b[38;5;241m.\u001b[39mdecay_sigma()\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# state = torch.tensor(state, dtype=torch.float32).to(device)\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/NEW/chargenv.py:135\u001b[0m, in \u001b[0;36mEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevs \u001b[38;5;241m=\u001b[39m \u001b[43mEV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEVS\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step_revenue \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_cs, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# self.ev = []\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/NEW/EV.py:109\u001b[0m, in \u001b[0;36mEVS.__init__\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path \u001b[38;5;241m=\u001b[39m data_path\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevs: Dict[\u001b[38;5;28mstr\u001b[39m, EV] \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# 存储所有车辆\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/NEW/EV.py:117\u001b[0m, in \u001b[0;36mEVS.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# 将时间字符串转换为datetime对象\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     connection_time \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconnectionTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    118\u001b[0m     done_charging_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoneChargingTime\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# 创建EV实例\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/_strptime.py:568\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strptime_datetime\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_string, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%a\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m    format string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m     tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m \u001b[43m_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m     tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    570\u001b[0m     args \u001b[38;5;241m=\u001b[39m tt[:\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m+\u001b[39m (fraction,)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/_strptime.py:369\u001b[0m, in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    367\u001b[0m weekday \u001b[38;5;241m=\u001b[39m julian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    368\u001b[0m found_dict \u001b[38;5;241m=\u001b[39m found\u001b[38;5;241m.\u001b[39mgroupdict()\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfound_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# Directives not explicitly handled below:\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m#   c, x, X\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m#      handled by making out of other directives\u001b[39;00m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;66;03m#   U, W\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;66;03m#      worthless without day of the week\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    376\u001b[0m         year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(found_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from chargenv import Env\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "\n",
    "        x = torch.tanh(self.fc2(x)) * self.action_bound\n",
    "        return x\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        cat = torch.cat([x, a], dim=1)  # 拼接状态和动作\n",
    "        x = F.relu(self.fc1(cat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc_out(x)\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    ''' DDPG算法 '''\n",
    "\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n",
    "        self.critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.actor.load_state_dict(torch.load(\"./model/ddpg_actor_initial5.pth\"))\n",
    "        self.critic.load_state_dict(torch.load(\"./model/ddpg_critic_initial5.pth\"))\n",
    "        self.target_actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n",
    "        self.target_critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 初始化目标价值网络并设置和价值网络相同的参数\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        # 初始化目标策略网络并设置和策略相同的参数\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr, weight_decay=0.01)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr, weight_decay=0.01)\n",
    "        self.gamma = gamma\n",
    "        self.sigma = sigma  # 高斯噪声的标准差,均值直接设为0\n",
    "        self.tau = tau  # 目标网络软更新参数\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "\n",
    "    def normalize(self, rewards):\n",
    "        \"\"\"\n",
    "        对奖励进行最大最小归一化\n",
    "        :param rewards: 奖励的列表或数组\n",
    "        :return: 归一化后的奖励\n",
    "        \"\"\"\n",
    "        min_reward = np.min(rewards.detach().cpu().numpy())\n",
    "        max_reward = np.max(rewards.detach().cpu().numpy())\n",
    "\n",
    "        normalized_rewards = (rewards - min_reward) / (max_reward - min_reward + 1e-10)  # 添加一个小常数防止除以零\n",
    "        return torch.tensor(normalized_rewards, requires_grad=True).to(device)\n",
    "\n",
    "    # def normalize(self, rewards):\n",
    "    #     \"\"\"\n",
    "    #     对奖励进行Z-Score标准化\n",
    "    #     :param rewards: 奖励的列表或数组\n",
    "    #     :return: 归一化后的奖励\n",
    "    #     \"\"\"\n",
    "    #     rewards_mean = np.mean(rewards.detach().cpu().numpy())\n",
    "    #     rewards_std = np.std(rewards.detach().cpu().numpy())\n",
    "    #\n",
    "    #     normalized_rewards = (rewards - rewards_mean) / (rewards_std + 1e-10)  # 添加一个小常数防止除以零\n",
    "    #     return torch.tensor(normalized_rewards, requires_grad=True).to(device)\n",
    "\n",
    "    def decay_sigma(self):\n",
    "        self.sigma *= 0.98\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = state.to(self.device)\n",
    "        action = self.actor(state).cpu()\n",
    "        # 给动作添加噪声，增加探索\n",
    "\n",
    "        action = action.detach() + self.sigma * torch.randn(self.action_dim)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.cat(transition_dict['states'], dim=0).to(torch.float).to(self.device)\n",
    "        actions = torch.cat(transition_dict['actions'], dim=0).to(torch.float).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        # rewards = self.normalize(rewards)\n",
    "        next_states = torch.cat(transition_dict['next_states'], dim=0).to(torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        next_q_values = self.target_critic(next_states, self.target_actor(next_states))\n",
    "        # next_q_values = self.normalize(next_q_values)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states, actions), q_targets))\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        actor_loss = -torch.mean(self.critic(states, self.actor(states)))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.actor, self.target_actor)  # 软更新策略网络\n",
    "        self.soft_update(self.critic, self.target_critic)  # 软更新价值网络\n",
    "        return critic_loss, actor_loss\n",
    "\n",
    "\n",
    "def train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device):\n",
    "    return_list = []\n",
    "    cl, al = 0, 0\n",
    "    max_reward = 0\n",
    "    for i_episode in range(num_episodes):\n",
    "        episode_return = 0\n",
    "        state = env.reset()\n",
    "        agent.decay_sigma()\n",
    "        # state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.take_action(state).to(device)\n",
    "            next_state, reward, done = env.step(torch.transpose(action, 0, 1))\n",
    "            # next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            # print(env.price, reward)\n",
    "\n",
    "            episode_return += reward.item()\n",
    "            if replay_buffer.size() > minimal_size:\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r, 'dones': b_d}\n",
    "                agent.update(transition_dict)\n",
    "                # agent.update(transition_dict)\n",
    "\n",
    "                # print(cl, al)\n",
    "        return_list.append(episode_return)\n",
    "        # if  episode_return<150:\n",
    "            # torch.save(agent.actor.state_dict(), './model/ddpg_actor_initial{}.pth'.format(arrival_rate))\n",
    "            # torch.save(agent.critic.state_dict(), './model/ddpg_critic_initial{}.pth'.format(arrival_rate))\n",
    "        if max_reward < episode_return:\n",
    "            # torch.save(agent.actor.state_dict(), './model/train/actor_ddpg{}.pth'.format(arrival_rate))\n",
    "            # torch.save(agent.critic.state_dict(), './model/train/critic_ddpg{}.pth'.format(arrival_rate))\n",
    "            # torch.save(agent.target_actor.state_dict(), './model/train/target_actor_ddpg{}.pth'.format(arrival_rate))\n",
    "            # torch.save(agent.target_critic.state_dict(), './model/train/target_critic_ddpg{}.pth'.format(arrival_rate))\n",
    "            max_reward = episode_return\n",
    "        if (i_episode + 1) % 10 == 0:\n",
    "            print(\n",
    "                'episodes: %d, reward: %f' % (i_episode + 1, torch.mean(torch.tensor(return_list[-10:], dtype=float))))\n",
    "            # print(env.price)\n",
    "\n",
    "        # print('---------------------------------------------')\n",
    "    return return_list\n",
    "\n",
    "\n",
    "actor_lr = 0.005\n",
    "critic_lr = 0.005\n",
    "num_episodes = 1000\n",
    "\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "tau = 0.001  # 软更新参数\n",
    "buffer_size = 40000\n",
    "\n",
    "minimal_size = 5000\n",
    "batch_size = 128\n",
    "sigma = 0.5  # 高斯噪声标准差\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "arrival_rate = 5\n",
    "data_path = './datasets/EVCD{}.csv'.format(arrival_rate)\n",
    "env = Env(1, 0, 0, arrival_rate, data_path)\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "state_dim = 4\n",
    "\n",
    "action_dim = env.n_cs\n",
    "action_bound = 0.5  # 动作最大值\n",
    "agent = DDPG(state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device)\n",
    "\n",
    "return_list = train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device)\n",
    "\n",
    "# episodes_list = list(range(len(return_list)))\n",
    "# plt.plot(episodes_list, return_list)\n",
    "# plt.xlabel('Episodes')\n",
    "# plt.ylabel('Returns')\n",
    "# plt.title('DDPG on DY')\n",
    "# plt.show()\n",
    "\n",
    "with open('./result/ddpg{}.csv'.format(arrival_rate), 'w', encoding='utf-8') as file:\n",
    "    for item in return_list:\n",
    "        file.write(f\"{item}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cec57aa-2c22-40ff-9748-7111a261a218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003.2810235023499\n",
      "848.2722661495209\n",
      "882.0003035068512\n",
      "856.2578642368317\n",
      "856.2578642368317\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "856.2578642368317\n",
      "856.2578642368317\n",
      "882.0003035068512\n",
      "856.2578642368317\n",
      "882.0003035068512\n",
      "874.0147054195404\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "856.2578642368317\n",
      "882.0003035068512\n",
      "856.2578642368317\n",
      "856.2578642368317\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "874.0147054195404\n",
      "882.0003035068512\n",
      "874.0147054195404\n",
      "870.6365988254547\n",
      "882.0003035068512\n",
      "882.0003035068512\n",
      "896.3790380954742\n",
      "882.0003035068512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 209\u001b[0m\n\u001b[1;32m    207\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPG(state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device)\n\u001b[1;32m    208\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 209\u001b[0m return_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_off_policy_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminimal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMR:\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28msum\u001b[39m(return_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(return_list),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;124m'\u001b[39m,(end_time\u001b[38;5;241m-\u001b[39mstart_time)\u001b[38;5;241m/\u001b[39mnum_episodes)\n",
      "Cell \u001b[0;32mIn[1], line 161\u001b[0m, in \u001b[0;36mtrain_off_policy_agent\u001b[0;34m(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    160\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtake_action(state)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 161\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# replay_buffer.add(state, action, reward, next_state, done)\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/autodl-tmp/NEW/chargenv.py:149\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m,action):\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step_revenue \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_cs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharging_vehicles)):  \u001b[38;5;66;03m# 遍历每个充电站\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from chargenv import Env\n",
    "import time\n",
    "import os\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "\n",
    "        x = torch.tanh(self.fc2(x)) * self.action_bound\n",
    "        return x\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        cat = torch.cat([x, a], dim=1)  # 拼接状态和动作\n",
    "        x = F.relu(self.fc1(cat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc_out(x)\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    ''' DDPG算法 '''\n",
    "\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n",
    "        self.critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n",
    "        self.target_critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.actor.load_state_dict(torch.load(\"./model/train/actor_ddpg%d.pth\" % arrival_rate))\n",
    "        self.critic.load_state_dict(torch.load(\"./model/train/critic_ddpg%d.pth\" % arrival_rate))\n",
    "        self.target_actor.load_state_dict(torch.load(\"./model/train/target_actor_ddpg%d.pth\" % arrival_rate))\n",
    "        self.target_critic.load_state_dict(torch.load(\"./model/train/target_critic_ddpg%d.pth\" % arrival_rate))\n",
    "        # # 初始化目标价值网络并设置和价值网络相同的参数\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        # 初始化目标策略网络并设置和策略相同的参数\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr, weight_decay=0.01)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr, weight_decay=0.01)\n",
    "        self.gamma = gamma\n",
    "        self.sigma = sigma  # 高斯噪声的标准差,均值直接设为0\n",
    "        self.tau = tau  # 目标网络软更新参数\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "\n",
    "    def normalize(self, rewards):\n",
    "        \"\"\"\n",
    "        对奖励进行最大最小归一化\n",
    "        :param rewards: 奖励的列表或数组\n",
    "        :return: 归一化后的奖励\n",
    "        \"\"\"\n",
    "        min_reward = np.min(rewards.detach().cpu().numpy())\n",
    "        max_reward = np.max(rewards.detach().cpu().numpy())\n",
    "\n",
    "        normalized_rewards = (rewards - min_reward) / (max_reward - min_reward + 1e-10)  # 添加一个小常数防止除以零\n",
    "        return torch.tensor(normalized_rewards, requires_grad=True).to(device)\n",
    "\n",
    "    # def normalize(self, rewards):\n",
    "    #     \"\"\"\n",
    "    #     对奖励进行Z-Score标准化\n",
    "    #     :param rewards: 奖励的列表或数组\n",
    "    #     :return: 归一化后的奖励\n",
    "    #     \"\"\"\n",
    "    #     rewards_mean = np.mean(rewards.detach().cpu().numpy())\n",
    "    #     rewards_std = np.std(rewards.detach().cpu().numpy())\n",
    "    #\n",
    "    #     normalized_rewards = (rewards - rewards_mean) / (rewards_std + 1e-10)  # 添加一个小常数防止除以零\n",
    "    #     return torch.tensor(normalized_rewards, requires_grad=True).to(device)\n",
    "\n",
    "    def decay_sigma(self):\n",
    "        self.sigma *= 0.98\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = state.to(self.device)\n",
    "        action = self.actor(state).cpu()\n",
    "        # 给动作添加噪声，增加探索\n",
    "        action = action.detach() + self.sigma * np.random.randn(self.action_dim)\n",
    "        return action\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.cat(transition_dict['states'], dim=0).to(torch.float).to(self.device)\n",
    "        actions = torch.cat(transition_dict['actions'], dim=0).to(torch.float).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        # rewards = self.normalize(rewards)\n",
    "        next_states = torch.cat(transition_dict['next_states'], dim=0).to(torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        next_q_values = self.target_critic(next_states, self.target_actor(next_states))\n",
    "        # next_q_values = self.normalize(next_q_values)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states, actions), q_targets))\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        actor_loss = -torch.mean(self.critic(states, self.actor(states)))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.actor, self.target_actor)  # 软更新策略网络\n",
    "        self.soft_update(self.critic, self.target_critic)  # 软更新价值网络\n",
    "        return critic_loss, actor_loss\n",
    "\n",
    "\n",
    "def train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device):\n",
    "    return_list = []\n",
    "    cl, al = 0, 0\n",
    "    max_reward = 0\n",
    "    for i_episode in range(num_episodes):\n",
    "        episode_return = 0\n",
    "        state = env.reset()\n",
    "        agent.decay_sigma()\n",
    "        # state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.take_action(state).to(device)\n",
    "            next_state, reward, done = env.step(torch.transpose(action, 0, 1))\n",
    "            # next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "            # replay_buffer.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            # print(env.price, reward)\n",
    "\n",
    "            episode_return += reward.item()\n",
    "            # if replay_buffer.size() > minimal_size:\n",
    "            #     b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "            #     transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r, 'dones': b_d}\n",
    "            #     agent.update(transition_dict)\n",
    "        return_list.append(episode_return)\n",
    "        # if max_reward < episode_return:\n",
    "        #     max_reward = episode_return\n",
    "        \n",
    "        print(episode_return)\n",
    "        # print('---------------------------------------------')\n",
    "    return return_list\n",
    "\n",
    "\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.001\n",
    "num_episodes = 100\n",
    "\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "tau = 0.001  # 软更新参数\n",
    "buffer_size = 10000\n",
    "\n",
    "minimal_size = 200\n",
    "batch_size = 128\n",
    "sigma = 0  # 高斯噪声标准差\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "arrival_rate = 1\n",
    "data_path = './datasets/EVCD{}.csv'.format(arrival_rate)\n",
    "env = Env(1, 0, 0, arrival_rate, data_path)\n",
    "\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "state_dim = 4\n",
    "\n",
    "action_dim = env.n_cs\n",
    "action_bound = 0.5  # 动作最大值\n",
    "agent = DDPG(state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device)\n",
    "start_time = time.time()\n",
    "return_list = train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device)\n",
    "end_time = time.time()\n",
    "print('MR:',sum(return_list) / len(return_list),'Time:',(end_time-start_time)/num_episodes)\n",
    "\n",
    "\n",
    "# with open('./result/test/DDPG{}.csv'.format(arrival_rate), 'w', encoding='utf-8') as file:\n",
    "#     for item in return_list:\n",
    "#         file.write(f\"{item}\\n\")\n",
    "        \n",
    "# os.makedirs(os.path.dirname('./result/test/DDPG{}.txt'.format(arrival_rate)), exist_ok=True)\n",
    "\n",
    "# 写入文件\n",
    "# with open('./result/test/DDPG{}.txt'.format(arrival_rate), 'w', encoding='utf-8') as file:\n",
    "#     file.write(f'Test_mode: DDPG{arrival_rate}, MR: {sum(return_list) / len(return_list)}, Time: {(end_time-start_time)/num_episodes}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618e6f3-89c9-4585-9577-157bb20d10d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7dfff2-90fe-478b-958e-b532cd1dd712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
