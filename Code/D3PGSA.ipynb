{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0998e388-26c1-4bd5-9b90-4b446ac6cad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodes: 10, reward: 165.780208\n",
      "episodes: 20, reward: 163.269290\n",
      "episodes: 30, reward: 163.011146\n",
      "episodes: 40, reward: 192.107413\n",
      "episodes: 50, reward: 196.663525\n",
      "episodes: 60, reward: 202.470552\n",
      "episodes: 70, reward: 170.456361\n",
      "episodes: 80, reward: 190.056116\n",
      "episodes: 90, reward: 168.893784\n",
      "episodes: 100, reward: 177.682407\n",
      "episodes: 110, reward: 169.674088\n",
      "episodes: 120, reward: 178.680290\n",
      "episodes: 130, reward: 179.822204\n",
      "episodes: 140, reward: 178.852478\n",
      "episodes: 150, reward: 169.637723\n",
      "episodes: 160, reward: 170.282537\n",
      "episodes: 170, reward: 183.612440\n",
      "episodes: 180, reward: 183.540196\n",
      "episodes: 190, reward: 183.780551\n",
      "episodes: 200, reward: 165.351042\n",
      "episodes: 210, reward: 179.086543\n",
      "episodes: 220, reward: 375.287060\n",
      "episodes: 230, reward: 526.654875\n",
      "episodes: 240, reward: 533.347036\n",
      "episodes: 250, reward: 532.730057\n",
      "episodes: 260, reward: 534.820509\n",
      "episodes: 270, reward: 536.165038\n",
      "episodes: 280, reward: 536.343437\n",
      "episodes: 290, reward: 533.340484\n",
      "episodes: 300, reward: 530.109292\n",
      "episodes: 310, reward: 537.048367\n",
      "episodes: 320, reward: 531.589136\n",
      "episodes: 330, reward: 542.371134\n",
      "episodes: 340, reward: 539.736571\n",
      "episodes: 350, reward: 536.846463\n",
      "episodes: 360, reward: 536.183234\n",
      "episodes: 370, reward: 533.679334\n",
      "episodes: 380, reward: 538.110366\n",
      "episodes: 390, reward: 542.828128\n",
      "episodes: 400, reward: 540.114812\n",
      "episodes: 410, reward: 536.923129\n",
      "episodes: 420, reward: 538.810437\n",
      "episodes: 430, reward: 544.452325\n",
      "episodes: 440, reward: 540.722775\n",
      "episodes: 450, reward: 529.464427\n",
      "episodes: 460, reward: 540.263440\n",
      "episodes: 470, reward: 539.596186\n",
      "episodes: 480, reward: 539.890395\n",
      "episodes: 490, reward: 533.725079\n",
      "episodes: 500, reward: 524.428261\n",
      "episodes: 510, reward: 530.476411\n",
      "episodes: 520, reward: 524.454332\n",
      "episodes: 530, reward: 535.176779\n",
      "episodes: 540, reward: 541.281100\n",
      "episodes: 550, reward: 531.192213\n",
      "episodes: 560, reward: 536.981736\n",
      "episodes: 570, reward: 537.338278\n",
      "episodes: 580, reward: 533.620655\n",
      "episodes: 590, reward: 533.707333\n",
      "episodes: 600, reward: 537.525437\n",
      "episodes: 610, reward: 537.152918\n",
      "episodes: 620, reward: 541.221748\n",
      "episodes: 630, reward: 546.215922\n",
      "episodes: 640, reward: 550.460834\n",
      "episodes: 650, reward: 546.393192\n",
      "episodes: 660, reward: 556.350540\n",
      "episodes: 670, reward: 549.564797\n",
      "episodes: 680, reward: 539.768985\n",
      "episodes: 690, reward: 539.586706\n",
      "episodes: 700, reward: 538.854896\n",
      "episodes: 710, reward: 537.420405\n",
      "episodes: 720, reward: 538.106297\n",
      "episodes: 730, reward: 538.000228\n",
      "episodes: 740, reward: 540.264915\n",
      "episodes: 750, reward: 548.336973\n",
      "episodes: 760, reward: 537.783503\n",
      "episodes: 770, reward: 541.164499\n",
      "episodes: 780, reward: 549.860321\n",
      "episodes: 790, reward: 545.877772\n",
      "episodes: 800, reward: 542.990802\n",
      "episodes: 810, reward: 538.306512\n",
      "episodes: 820, reward: 540.617490\n",
      "episodes: 830, reward: 529.214010\n",
      "episodes: 840, reward: 527.878954\n",
      "episodes: 850, reward: 531.550930\n",
      "episodes: 860, reward: 536.869753\n",
      "episodes: 870, reward: 545.338307\n",
      "episodes: 880, reward: 540.250927\n",
      "episodes: 890, reward: 548.391442\n",
      "episodes: 900, reward: 541.552216\n",
      "episodes: 910, reward: 527.215923\n",
      "episodes: 920, reward: 541.557877\n",
      "episodes: 930, reward: 535.948256\n",
      "episodes: 940, reward: 537.189265\n",
      "episodes: 950, reward: 534.218801\n",
      "episodes: 960, reward: 536.666795\n",
      "episodes: 970, reward: 532.719079\n",
      "episodes: 980, reward: 541.229615\n",
      "episodes: 990, reward: 549.039612\n",
      "episodes: 1000, reward: 541.402475\n",
      "568.4555426836014\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch.nn.functional as F\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from chargenv5 import Env\n",
    "from experience_generation_model import main\n",
    "from feature_generation_model import simsiam\n",
    "from collections import deque\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, eps=1, min_samples=5):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        self.clusters = {}\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "\n",
    "    def add(self, experience):\n",
    "        # experience is expected to be a tuple (feature_vector, action, reward, next_feature_vector, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def cluster(self, features):\n",
    "        # Assumes the first element of each experience in the buffer is the feature vector for clustering\n",
    "\n",
    "        if len(features) == 0:\n",
    "            return\n",
    "        labels = DBSCAN(eps=self.eps, min_samples=self.min_samples).fit_predict(features.cpu())\n",
    "        self.clusters = {}  # Reset clusters\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in self.clusters:\n",
    "                self.clusters[label] = []\n",
    "            self.clusters[label].append(self.buffer[i])\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Ensure there's at least one sample per cluster\n",
    "        num_clusters = len(self.clusters)\n",
    "        if num_clusters == 0:\n",
    "            return []\n",
    "\n",
    "        samples_per_cluster = max(1, batch_size // num_clusters)\n",
    "        samples = []\n",
    "\n",
    "        for cluster in self.clusters.values():\n",
    "            if len(cluster) < samples_per_cluster:\n",
    "                samples += cluster\n",
    "            else:\n",
    "                samples += random.sample(cluster, samples_per_cluster)\n",
    "\n",
    "        # If we don't have enough samples due to rounding, sample from the entire buffer\n",
    "        while len(samples) < batch_size:\n",
    "            samples.append(random.choice(self.buffer))\n",
    "\n",
    "        return samples[:batch_size]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, capacity):\n",
    "#         self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "#     def add(self, e):\n",
    "#         self.buffer.append(e)\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         transitions = random.sample(self.buffer, batch_size)\n",
    "#         return transitions\n",
    "\n",
    "#     def size(self):\n",
    "#         return len(self.buffer)\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 256)\n",
    "        self.fc3 = torch.nn.Linear(256, 64)\n",
    "        self.fc4 = torch.nn.Linear(64, action_dim)\n",
    "        self.action_bound = action_bound\n",
    "        # init.normal_(self.fc1.weight, mean=0.5, std=0.01)\n",
    "        # init.zeros_(self.fc1.bias)\n",
    "        #\n",
    "        # # 初始化第二个全连接层\n",
    "        # init.normal_(self.fc2.weight, mean=0.5, std=0.01)\n",
    "        # init.zeros_(self.fc2.bias)\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x)) * self.action_bound\n",
    "        return x\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 256)\n",
    "        self.fc3 = torch.nn.Linear(256, 64)\n",
    "        self.fc_out = torch.nn.Linear(64, 1)\n",
    "        # init.normal_(self.fc1.weight, mean=0.0, std=0.01)\n",
    "        # init.zeros_(self.fc1.bias)\n",
    "        #\n",
    "        # # 初始化第二个全连接层\n",
    "        # init.normal_(self.fc2.weight, mean=0.0, std=0.01)\n",
    "        # init.zeros_(self.fc2.bias)\n",
    "        # init.normal_(self.fc_out.weight, mean=0.0, std=0.01)\n",
    "        # init.zeros_(self.fc_out.bias)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        cat = torch.cat([x, a], dim=1) # 拼接状态和动作\n",
    "        x = F.relu(self.fc1(cat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc_out(x)\n",
    "\n",
    "class DDPG:\n",
    "    ''' DDPG算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n",
    "        self.critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.lr = actor_lr\n",
    "        \n",
    "        self.actor.load_state_dict(torch.load(\"./model/actor_initial5.pth\"))\n",
    "        self.critic.load_state_dict(torch.load(\"./model/critic_initial5.pth\" ))\n",
    "        self.critic2.load_state_dict(torch.load(\"./model/critic2_initial5.pth\" ))\n",
    "        # init.normal_(self.actor.weight, mean=0.0, std=0.01)\n",
    "        # init.zeros_(self.actor.bias)\n",
    "        self.target_actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n",
    "        self.target_critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # self.actor.load_state_dict(torch.load(\"./model/train/actor_DCE-DDPG%d_0.002.pth\" % arrival_rate))\n",
    "        # self.critic.load_state_dict(torch.load(\"./model/train/critic_DCE-DDPG%d_0.002.pth\" % arrival_rate))\n",
    "        # self.critic2.load_state_dict(torch.load(\"./model/train/critic2_DCE-DDPG%d_0.002.pth\" % arrival_rate))\n",
    "        # self.target_actor.load_state_dict(torch.load(\"./model/train/target_actor_DCE-DDPG%d_0.002.pth\" % arrival_rate))\n",
    "        # self.target_critic.load_state_dict(torch.load(\"./model/train/target_critic_DCE-DDPG%d_0.002.pth\" % arrival_rate))\n",
    "        # self.target_critic2.load_state_dict(torch.load(\"./model/train/target_critic2_DCE-DDPG%d_0.002.pth\" % arrival_rate))\n",
    "        # 初始化目标价值网络并设置和价值网络相同的参数\n",
    "\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.lr, weight_decay=0.001)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.lr, weight_decay=0.001)\n",
    "        self.critic2_optimizer = torch.optim.Adam(self.critic2.parameters(), lr=self.lr, weight_decay=0.001)\n",
    "\n",
    "        # 初始化第二个目标评论者网络的参数\n",
    "\n",
    "        # 第二个评论者网络的优化器\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.sigma = sigma  # 高斯噪声的标准差,均值直接设为0\n",
    "        self.tau = tau  # 目标网络软更新参数\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def decay_sigma(self):\n",
    "        self.sigma *= 0.98\n",
    "    def decay_lr(self):\n",
    "        self.lr *= 0.5\n",
    "    def take_action(self, state):\n",
    "        state = state.to(self.device)\n",
    "        action = self.actor(state).cpu()\n",
    "        # 给动作添加噪声，增加探索\n",
    "        action = action.detach() + self.sigma * torch.randn(self.action_dim, dtype=torch.float32)\n",
    "        return action\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        stacked_tensors = torch.cat((transition_dict), dim=1).squeeze(0).to(self.device)  # 移除中间的维度，得到 64x11\n",
    "        states = stacked_tensors[:, :4]  # 结果形状为 64x4\n",
    "        actions = stacked_tensors[:, 4:5]  # 结果形状为 64x1\n",
    "        rewards = stacked_tensors[:, 5:6]  # 结果形状为 64x1\n",
    "        next_states = stacked_tensors[:, 6:10]  # 结果形状为 64x4\n",
    "        dones = stacked_tensors[:, 10:11]  # 结果形状为 64x1\n",
    "        # next_q_values = self.target_critic(next_states, self.target_actor(next_states))\n",
    "        # next_q_values = self.normalize(next_q_values)\n",
    "        target_Q1 = self.target_critic(next_states, self.target_actor(next_states))\n",
    "        target_Q2 = self.target_critic2(next_states, self.target_actor(next_states))\n",
    "        target_Q = torch.min(target_Q1, target_Q2)\n",
    "        q_targets = rewards + self.gamma * target_Q * (1 - dones)\n",
    "        # q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        critic_loss = F.mse_loss(self.critic(states, actions), q_targets, reduction='none')\n",
    "        td_error1 = critic_loss.clone()\n",
    "        critic_loss = (critic_loss).mean()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward(retain_graph=True)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        critic2_loss = F.mse_loss(self.critic2(states, actions), q_targets, reduction='none')\n",
    "        td_error2 = critic2_loss.clone()\n",
    "        critic2_loss = (critic2_loss).mean()\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # 在soft_update方法中也要更新第二个目标评论者网络\n",
    "        if torch.equal(target_Q, target_Q1):\n",
    "            actor_loss = -torch.mean(self.critic(states, self.actor(states)))\n",
    "        else :\n",
    "            actor_loss = -torch.mean(self.critic2(states, self.actor(states)))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.actor, self.target_actor)  # 软更新策略网络\n",
    "        self.soft_update(self.critic, self.target_critic)  # 软更新价值网络\n",
    "        self.soft_update(self.critic2, self.target_critic2)\n",
    "\n",
    "        \n",
    "\n",
    "def train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device):\n",
    "    return_list = []\n",
    "    max_reward = 0\n",
    "    first = True\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        episode_return = 0\n",
    "        state = env.reset()\n",
    "        # if i%10==0:\n",
    "        agent.decay_sigma()\n",
    "        # state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.take_action(state).to(device)\n",
    "            next_state, reward, done = env.step(torch.transpose(action, 0, 1))\n",
    "            # next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "            experience = torch.cat((state, action, reward.view(1,1), next_state,\n",
    "                                    torch.tensor(done, dtype=torch.float32, device=device).view(1, 1)),dim=1)\n",
    "            replay_buffer.add(experience.unsqueeze(1))\n",
    "\n",
    "            state = next_state\n",
    "            # print(env.price, reward)\n",
    "\n",
    "            episode_return += reward.item()\n",
    "            if replay_buffer.size() > minimal_size:\n",
    "                if first:\n",
    "                    if os.path.isfile('./model/cm_model%d.pth' % arrival_rate):\n",
    "                        E = main.generate(arrival_rate)\n",
    "                        E = torch.cat(E,dim=0).squeeze(1)\n",
    "\n",
    "                        for i in range(E.size(0)):  # 遍历500\n",
    "                            for j in range(E.size(1)):  # 遍历24\n",
    "                                E[i, j, 3] = j+1\n",
    "                                E[i, j, 9] = j+1\n",
    "                                if j != E.size(1) - 1:\n",
    "                                    E[i, j, -1] = 0  # 如果不是最后一个1x11的tensor，则最后一个值改为0\n",
    "                                else:\n",
    "                                    E[i, j, -1] = 1  # 如果是最后一个1x11的tensor，则最后一个值改为1\n",
    "                                # 将修改后的 tensor 添加到经验池中\n",
    "                                replay_buffer.add(E[i, j].unsqueeze(0).unsqueeze(0))\n",
    "                                first = False\n",
    "                        del E\n",
    "                        # simsiam.train(replay_buffer.buffer, arrival_rate)\n",
    "                        model = simsiam.test(arrival_rate)\n",
    "                        features = model(torch.cat(list(replay_buffer.buffer), dim=0))[-1]\n",
    "                        replay_buffer.cluster(features)\n",
    "                        del model\n",
    "                    else:\n",
    "                        e = replay_buffer.sample(minimal_size)\n",
    "                        num_tensors_to_concatenate = len(e) - (len(e) % 24)\n",
    "                        data = [torch.cat(e[i:i + 24], dim=1) for i in range(0, num_tensors_to_concatenate, 24)]\n",
    "                        main.train(data, arrival_rate)\n",
    "\n",
    "                        E = main.generate(arrival_rate)\n",
    "                        E = torch.cat(E, dim=0).squeeze(1)\n",
    "                        for i in range(E.size(0)):  # 遍历500\n",
    "                            for j in range(E.size(1)):  # 遍历24\n",
    "                                if j != E.size(1) - 1:\n",
    "                                    E[i, j, -1] = 0  # 如果不是最后一个1x11的tensor，则最后一个值改为0\n",
    "                                else:\n",
    "                                    E[i, j, -1] = 1  # 如果是最后一个1x11的tensor，则最后一个值改为1\n",
    "                                # 将修改后的 tensor 添加到经验池中\n",
    "                                replay_buffer.add(E[i, j].unsqueeze(0).unsqueeze(0))\n",
    "                                first = False\n",
    "                        simsiam.train(replay_buffer.buffer, arrival_rate)\n",
    "                        model = simsiam.test(arrival_rate)\n",
    "                        features = model(torch.cat(list(replay_buffer.buffer), dim=0))[-1]\n",
    "                        replay_buffer.cluster(features)\n",
    "                        del model\n",
    "                else:\n",
    "                    e = replay_buffer.sample(batch_size)\n",
    "                    agent.update(e)\n",
    "                    # agent.update(e)\n",
    "\n",
    "                # transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r, 'dones': b_d, 'weight':w, 'index':i}\n",
    "                # agent.update(transition_dict)\n",
    "                # print(cl, al)\n",
    "        return_list.append(episode_return)\n",
    "        # if episode_return<150:\n",
    "        #     torch.save(agent.actor.state_dict(), './model/actor_initial{}.pth'.format(arrival_rate))\n",
    "        #     torch.save(agent.critic.state_dict(), './model/critic_initial{}.pth'.format(arrival_rate))\n",
    "        #     torch.save(agent.critic2.state_dict(), './model/critic2_initial{}.pth'.format(arrival_rate))\n",
    "        if max_reward < episode_return:\n",
    "            # torch.save(agent.actor.state_dict(),'./model/train/actor_DCE-DDPG{}_{}.pth'.format(arrival_rate, actor_lr))\n",
    "            # torch.save(agent.critic.state_dict(), './model/train/critic_DCE-DDPG{}_{}.pth'.format(arrival_rate, actor_lr))\n",
    "            # torch.save(agent.critic2.state_dict(), './model/train/critic2_DCE-DDPG{}_{}.pth'.format(arrival_rate, actor_lr))\n",
    "            # torch.save(agent.target_actor.state_dict(), './model/train/target_actor_DCE-DDPG{}_{}.pth'.format(arrival_rate, actor_lr))\n",
    "            # torch.save(agent.target_critic.state_dict(), './model/train/target_critic_DCE-DDPG{}_{}.pth'.format(arrival_rate, actor_lr))\n",
    "            # torch.save(agent.target_critic2.state_dict(), './model/train/target_critic2_DCE-DDPG{}_{}.pth'.format(arrival_rate, actor_lr))\n",
    "            max_reward = episode_return\n",
    "        if (i_episode + 1) % 10 == 0:\n",
    "            print('episodes: %d, reward: %f' % (i_episode + 1, torch.mean(torch.tensor(return_list[-10:],dtype=float))))\n",
    "            if replay_buffer.size() > minimal_size:\n",
    "                model = simsiam.test(arrival_rate)\n",
    "                features = model(torch.cat(list(replay_buffer.buffer), dim=0))[-1]\n",
    "                replay_buffer.cluster(features)\n",
    "                del model\n",
    "        if (i_episode + 1) % 500 == 0:\n",
    "            agent.decay_lr()\n",
    "    return return_list,max_reward\n",
    "\n",
    "\n",
    "actor_lr = 0.002\n",
    "critic_lr = 0.002\n",
    "num_episodes = 1000\n",
    "\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "tau = 0.001  # 软更新参数\n",
    "buffer_size = 40000\n",
    "\n",
    "minimal_size = 5000\n",
    "batch_size = 128\n",
    "sigma = 0.5  # 高斯噪声标准差\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "arrival_rate = 5\n",
    "dataset = 'ACN-Data'\n",
    "data_path = './datasets/{}.csv'.format(dataset)\n",
    "env = Env(1, 0, 0, arrival_rate, data_path)\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "state_dim = 4\n",
    "\n",
    "action_dim = env.n_cs\n",
    "action_bound = 0.5  # 动作最大值\n",
    "#\n",
    "agent = DDPG(state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device)\n",
    "\n",
    "return_list, MR = train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device)\n",
    "print(MR)\n",
    "\n",
    "# episodes_list = list(range(len(return_list)))\n",
    "# plt.plot(episodes_list, return_list)\n",
    "# plt.xlabel('Episodes')\n",
    "# plt.ylabel('Returns')\n",
    "# plt.title('DCE-DDPG on DY')\n",
    "# plt.show()\n",
    "\n",
    "# with open('./result/DCE-DDPG{}_0.002.csv'.format(arrival_rate), 'w', encoding='utf-8') as file:\n",
    "#     for item in return_list:\n",
    "#         file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "999bed13-a8e7-4b85-b45d-7179b3da928f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592.2731876373291\n",
      "633.5940645933151\n",
      "627.1296588182449\n",
      "610.1912013292313\n",
      "612.7230118513107\n",
      "619.0970069169998\n",
      "617.5239306688309\n",
      "625.1366075277328\n",
      "604.0409675836563\n",
      "627.1296588182449\n",
      "619.6823068857193\n",
      "618.6710284948349\n",
      "617.351046204567\n",
      "623.9018400907516\n",
      "617.4403344392776\n",
      "616.6770249605179\n",
      "613.803253531456\n",
      "626.0424035787582\n",
      "627.7026447057724\n",
      "627.7049292325974\n",
      "611.1629086732864\n",
      "619.9348102807999\n",
      "620.1604119539261\n",
      "607.3999184370041\n",
      "624.5854164361954\n",
      "627.1827713251114\n",
      "598.1810766458511\n",
      "607.5437880754471\n",
      "616.3633319139481\n",
      "612.7159358263016\n",
      "624.9088214635849\n",
      "620.1604119539261\n",
      "627.124459862709\n",
      "621.4955102205276\n",
      "608.0405181646347\n",
      "630.4938875436783\n",
      "621.2438470125198\n",
      "613.8103266954422\n",
      "619.9556204080582\n",
      "617.351046204567\n",
      "621.4903093576431\n",
      "599.4374731779099\n",
      "623.7321172952652\n",
      "614.0456749200821\n",
      "626.0495473146439\n",
      "624.6098538637161\n",
      "613.5537391901016\n",
      "620.1604119539261\n",
      "614.8847645521164\n",
      "616.2689934968948\n",
      "621.2438470125198\n",
      "613.5499528646469\n",
      "626.6979492902756\n",
      "609.0554655790329\n",
      "621.01675760746\n",
      "616.6127246618271\n",
      "610.7709695100784\n",
      "627.1320778131485\n",
      "621.2438470125198\n",
      "613.909769654274\n",
      "612.8681474924088\n",
      "636.4094091653824\n",
      "624.9088214635849\n",
      "622.5789452791214\n",
      "623.2401815652847\n",
      "618.6050711870193\n",
      "619.9578667879105\n",
      "613.2135518789291\n",
      "618.7818804979324\n",
      "604.414513707161\n",
      "629.7698649168015\n",
      "611.157540678978\n",
      "607.400507569313\n",
      "621.2438470125198\n",
      "617.2783635854721\n",
      "623.2401815652847\n",
      "604.046170592308\n",
      "628.4680806398392\n",
      "619.7623461484909\n",
      "621.2438470125198\n",
      "621.316523194313\n",
      "625.9438143968582\n",
      "617.0310159921646\n",
      "627.710995554924\n",
      "626.1885055303574\n",
      "621.2438470125198\n",
      "611.6562620401382\n",
      "621.2452176809311\n",
      "627.7040122747421\n",
      "621.2386461496353\n",
      "603.873050570488\n",
      "611.1629086732864\n",
      "613.5537391901016\n",
      "608.5006450414658\n",
      "598.4812644720078\n",
      "627.1589316129684\n",
      "618.9481703042984\n",
      "620.158466219902\n",
      "614.4962745904922\n",
      "618.7818804979324\n",
      "MR: 618.1808048284054 Time: 40.63105895280838\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch.nn.functional as F\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from chargenv import Env\n",
    "import simsiam\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 256)\n",
    "        self.fc3 = torch.nn.Linear(256, 64)\n",
    "        self.fc4 = torch.nn.Linear(64, action_dim)\n",
    "        self.action_bound = action_bound\n",
    "        # init.normal_(self.fc1.weight, mean=0.5, std=0.01)\n",
    "        # init.zeros_(self.fc1.bias)\n",
    "        #\n",
    "        # # 初始化第二个全连接层\n",
    "        # init.normal_(self.fc2.weight, mean=0.5, std=0.01)\n",
    "        # init.zeros_(self.fc2.bias)\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x)) * self.action_bound\n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    ''' DDPG算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)    \n",
    "        self.actor.load_state_dict(torch.load(\"./model/train/actor_DCE-DDPG{}_{}.pth\".format(arrival_rate, actor_lr)))\n",
    "        self.action_dim = action_dim\n",
    "    def take_action(self, state):\n",
    "        state = state.to(device)\n",
    "        action = self.actor(state)\n",
    "        # 给动作添加噪声，增加探索\n",
    "        # action = action.detach() + self.sigma * torch.randn(self.action_dim, dtype=torch.float32)\n",
    "        return action\n",
    "\n",
    "        \n",
    "\n",
    "def train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device):\n",
    "    return_list = []\n",
    "    max_reward = 0\n",
    "    first = True\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        episode_return = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        # state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.take_action(state).to(device)\n",
    "            next_state, reward, done = env.step(torch.transpose(action, 0, 1))\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            episode_return += reward.item()\n",
    "            \n",
    "        return_list.append(episode_return)\n",
    "        print(episode_return)\n",
    "    return return_list,max_reward\n",
    "\n",
    "\n",
    "\n",
    "actor_lr = 0.002\n",
    "critic_lr = 0.001\n",
    "num_episodes = 100\n",
    "\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "tau = 0.001  # 软更新参数\n",
    "buffer_size = 10000\n",
    "\n",
    "minimal_size = 300\n",
    "batch_size = 128\n",
    "sigma = 0.1 # 高斯噪声标准差\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "arrival_rate = 6\n",
    "data_path = './datasets/EVCD{}.csv'.format(arrival_rate)\n",
    "env = Env(1, 0, 0, arrival_rate, data_path)\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "state_dim = 4\n",
    "\n",
    "action_dim = env.n_cs\n",
    "action_bound = 0.5  # 动作最大值\n",
    "agent = DDPG(state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device)\n",
    "\n",
    "\n",
    "\n",
    "return_list, MR = train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size, device)\n",
    "end_time = time.time()\n",
    "print('MR:',sum(return_list) / (len(return_list)),'Time:',(end_time-start_time)/num_episodes)\n",
    "\n",
    "\n",
    "\n",
    "# os.makedirs(os.path.dirname('./result/test/DCE-DDPG{}.txt'.format(arrival_rate)), exist_ok=True)\n",
    "#\n",
    "# # 写入文件\n",
    "# with open('./result/test/DCE-DDPG{}.txt'.format(arrival_rate), 'w', encoding='utf-8') as file:\n",
    "#     file.write(f'Test_mode: DCE-DDPG{arrival_rate}, MR: {sum(return_list) / len(return_list)}, Time: {(end_time-start_time)/num_episodes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b088c82a-b075-4b71-9b88-991da3a22183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832903be-7d16-4238-8feb-a9167749f378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
